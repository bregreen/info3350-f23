{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 02: Dictionary methods\n",
    "\n",
    "## To do\n",
    "\n",
    "* Readings\n",
    "    * \"Emotional Arcs ...\" and others for today\n",
    "    * Two articles and one project to read/review for Wednesday\n",
    "* **Response posts.** See [Canvas for schedule and instructions](https://canvas.cornell.edu/courses/57246/discussion_topics/618217). \n",
    "    * The readings themselves are *always* required. Read every week. Respond only as scheduled.\n",
    "    * This week: NetIDs `a*-g*`\n",
    "* PS0 due Thursday night at 11:59pm\n",
    "    * If you're still on the waitlist, you can hold off on PS0. We'll accept it without penalty after you're admited.\n",
    "* Speaking of the waitlist ...\n",
    "    * Keep attending lecture if you want to be admitted. I'll have an update on Wednesday.\n",
    "* Remember that Ed is the place for questions about any aspect of the course. \n",
    "    * Follow up in office hours as needed.\n",
    "    \n",
    "## What are dictionary methods and what do they have to do with word counts?\n",
    "\n",
    "Dictionary methods use a *dictionary* or *lexicon* to measure textual properties on the basis of known values associated with words or phrases. You can think of them as a pre-determined mapping between words and features.\n",
    "\n",
    "### Example: How much \"animalness\" does a text contain?\n",
    "\n",
    "The [text](https://helenadailyenglish.com/short-stories-in-english-a-cat-and-a-dog.html):\n",
    "\n",
    "> The black kitten jumped up onto the chair. It looked down at the white puppy. The dog was chewing on a bone. The cat jumped onto the dog. The dog kept chewing the bone. The cat played with the dogâ€™s tail. The dog kept chewing the bone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from   collections import Counter\n",
    "from   nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "story = '''The black kitten jumped up onto the chair. \n",
    "It looked down at the white puppy. \n",
    "The dog was chewing on a bone. \n",
    "The cat jumped onto the dog. \n",
    "The dog kept chewing the bone. \n",
    "The cat played with the dog's tail. \n",
    "The dog kept chewing the bone.\n",
    "A bird looked on.\n",
    "'''\n",
    "\n",
    "# case fold and tokenize, then remove punctuation\n",
    "story_tokens = Counter(word_tokenize(story.lower()))\n",
    "for i in string.punctuation:\n",
    "    del story_tokens[i]\n",
    "\n",
    "# word count\n",
    "story_token_count = sum(story_tokens.values())\n",
    "print(\"Story tokens:\", story_token_count)\n",
    "\n",
    "# count cat words\n",
    "cat_token_set = set(['cat', 'kitty', 'kitten', 'feline'])\n",
    "cat_token_count = sum([story_tokens[i] for i in cat_token_set])\n",
    "print(\"Cat tokens:\", cat_token_count)\n",
    "\n",
    "# Calculate \"animalness\" = cat tokens / all tokens\n",
    "print(f\"'Animalness': {100* cat_token_count / story_token_count:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a more comprehensive list of animals ...\n",
    "animal_token_set = set(['cat', 'kitty', 'kitten', 'feline', 'dog', 'puppy', 'bird', 'chick'])\n",
    "\n",
    "animal_token_count = sum([story_tokens[i] for i in animal_token_set])\n",
    "print(\"Animal tokens:\", animal_token_count)\n",
    "\n",
    "# Calculate \"animalness\"\n",
    "print(f\"'Animalness': {100* animal_token_count / story_token_count:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems with this method?\n",
    "\n",
    "Take 10 seconds to consider limitations or problems with this method of measuring the \"animalness\" of a story. Then put your thoughts in the chat.\n",
    "\n",
    "How might one try to address these issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A common use case: sentiment and emotion\n",
    "\n",
    "We want to determine the *sentiment* of a text and of the individual sentences from which the text is composed.\n",
    "\n",
    "\"Sentiment\" can mean a lot of things:\n",
    "\n",
    "* Positive and negative feelings\n",
    "* Emotional intensity (could be good or bad)\n",
    "* Amount or intensity of other emotions (joy, surprise, awe, fear, etc.)\n",
    "* Maybe even \"sentimentalness\" (roughly, \"nostalgia\")\n",
    "\n",
    "Today, we'll focus on sentiment as the expression of positive and negative feelings at the token level. This is a common case in many text anlaysis problems, though it has enough problems that you probably shouldn't use it for any sort of general-purpose problem.\n",
    "\n",
    "## Supervised and unsupervised\n",
    "\n",
    "There are two broad ways we could approach the task of senitment analysis:\n",
    "\n",
    "* **Unsupervised** methods start with **known-informative features** and produce labels or scores from those features.\n",
    "* **Supervised** methods start with **labeled data** and try to learn the features that best predict the labels.\n",
    "* Advantages and disadvantages of each\n",
    "  * In short: upfront labeling costs vs. later validation costs\n",
    "  \n",
    "## Warning\n",
    "\n",
    "Every semester, we see a surprising number of student exams/projects that are built around unsupervised sentiment analysis. These projects tend to be weak, since they rely on a method covered in lecture 2 that has lots of known limitations and many superior alternatives. Do not fall into this trap.\n",
    "\n",
    "## Our method\n",
    "\n",
    "We will work, for now, with **unsupervised** sentiment analysis. But there are lots of supervised approaches, too.\n",
    "\n",
    "Specifically, we're going to use lexical (dictionary-based) methods that assign one or more emotions to a subset of English words. We will assume that each of those words in a given text is an indication that the text contains the corresponding emotion. We can them sum up the emotions over all words in the text to get a measurement of that text's net emotional content.\n",
    "\n",
    "**Quick exercise:** Do you expect this method to work? Do you think there are cases where it might work especially well or poorly? What are some potential problems?\n",
    "\n",
    "## Example case\n",
    "\n",
    "* From Jockers' [*Syuzhet* vignette](https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html). \n",
    " * Note that this is an R package. We can't use it directly.\n",
    "* \"Syuzhet\" = \"plot\" or \"subject\" in Russian; it refers to narrative order, rather than to the \"true\" order of the narrative's underlying events (which is called the *fabula*). See [Russian Formalism](https://en.wikipedia.org/wiki/Russian_formalism).\n",
    "* To see the difference, think about a flashback that occurs near the end of a story.\n",
    "\n",
    "Consider the following story (or \"story\"):\n",
    "\n",
    "> I begin this story with a neutral statement.  \n",
    "  Basically this is a very silly test.  \n",
    "  You are testing the Syuzhet package using short, inane sentences.  \n",
    "  I am actually very happy today.  \n",
    "  I have finally finished writing this package.  \n",
    "  Tomorrow I will be very sad.  \n",
    "  I won't have anything left to do.  \n",
    "  I might get angry and decide to do something horrible.  \n",
    "  I might destroy the entire package and start from scratch.  \n",
    "  Then again, I might find it satisfying to have completed my first R package.  \n",
    "  Honestly this use of the Fourier transformation is really quite elegant.  \n",
    "  You might even say it's beautiful!  \n",
    "\n",
    "### Score some sentences ...\n",
    "\n",
    "By show of hands, how many think each sentence is:\n",
    "* Positive\n",
    "* Negative\n",
    "\n",
    "The sentences:\n",
    "1. **Basically this is a very silly test.**\n",
    "1. **You are testing the Syuzhet package using short, inane sentences.**  \n",
    "1. **I have finally finished writing this package.**  \n",
    "1. **I won't have anything left to do.** \n",
    "1. **Honestly this use of the Fourier transformation is really quite elegant.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enter eyeballed values from in-class survey ...\n",
    "# Range [-2, 2] strong negative to strong positive\n",
    "silly    = 0.\n",
    "inane    = 0.\n",
    "finished = 0.\n",
    "left     = -0.75\n",
    "fourier  = 1.7\n",
    "\n",
    "# scores for all sentences, includings ones not recorded above\n",
    "human_scores = [\n",
    "    0,\n",
    "    silly,\n",
    "    inane,\n",
    "    2,\n",
    "    finished,\n",
    "    -2,\n",
    "    left,\n",
    "    -2,\n",
    "    -2,\n",
    "    1,\n",
    "    fourier,\n",
    "    1.5\n",
    "]\n",
    "\n",
    "print(\"Human scores by sentence:\", human_scores)\n",
    "print(\"Overall human sentiment score:\", round(sum(human_scores),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confidence check:** Does this strike us as a reasonable summary of the overall positive-negative affect of the sample story? If not, why not?\n",
    "\n",
    "### Ingest and tokenize the example \"story\"\n",
    "\n",
    "Notice that our output data structure is a list of lists. The \"outer\" list contains sentences. The \"inner\" lists each contain the tokens in one sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "# The story. Why triple quotes?\n",
    "story = '''\\\n",
    "  I begin this story with a neutral statement.  \n",
    "  Basically this is a very silly test.  \n",
    "  You are testing the Syuzhet package using short, inane sentences.  \n",
    "  I am actually very happy today. \n",
    "  I have finally finished writing this package.  \n",
    "  Tomorrow I will be very sad. \n",
    "  I won't have anything left to do. \n",
    "  I might get angry and decide to do something horrible.  \n",
    "  I might destroy the entire package and start from scratch.  \n",
    "  Then again, I might find it satisfying to have completed my first R package. \n",
    "  Honestly this use of the Fourier transformation is really quite elegant.  \n",
    "  You might even say it's beautiful!'''\n",
    "\n",
    "# Tokenize the story\n",
    "tokens = [word_tokenize(sent.lower()) for sent in sent_tokenize(story)]\n",
    "print(\"Sentences:\", len(tokens))\n",
    "print(\"Total tokens:\", sum([len(sent) for sent in tokens]))\n",
    "print(\"\\nSample sentences:\", tokens[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up sentiment dictionaries\n",
    "\n",
    "We want to compare a couple of them. Specifically, we'll use:\n",
    "\n",
    "* NLTK's copy of Hu and Liu's lexicon ([paper](https://www.cs.uic.edu/~liub/publications/kdd04-revSummary.pdf) | [dataset](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html))\n",
    "* Mohammad's [EmoLex](http://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm) from the Canadian National Research Council (NRC). \n",
    "    * Crowd-sourced word associations.\n",
    "    * Some are ... questionable?\n",
    "      * \"fall\" -> `SADNESS`; \"pregnancy\" -> `DISGUST|NEGATIVE`\n",
    "\n",
    "Are these good and suitable choices for our task? Let's (begin to) find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from   collections import defaultdict\n",
    "import nltk\n",
    "import os\n",
    "import random\n",
    "\n",
    "nltk.download('opinion_lexicon') # Need to download this the first time used\n",
    "\n",
    "# NLTK simple lexicon (from Hu and Liu (2004))\n",
    "nltk_lexicon = {\n",
    "    'positive' : set(nltk.corpus.opinion_lexicon.positive()), # Why cast to a set?\n",
    "    'negative' : set(nltk.corpus.opinion_lexicon.negative())\n",
    "}\n",
    "\n",
    "# Print a sample of the NLTK lexicon\n",
    "print('NLTK lexicon sample')\n",
    "for key in nltk_lexicon.keys():\n",
    "    print(f'{key}:', random.sample(tuple(nltk_lexicon[key]), 5))\n",
    "    \n",
    "# NRC EmoLex lexicon (from Mohammad, http://sentiment.nrc.ca/lexicons-for-research/)\n",
    "# No package for this, just read the data from a local file\n",
    "emolex_file = os.path.join('..', 'data', 'lexicons', 'emolex.txt')\n",
    "nrc_lexicon = defaultdict(dict) # Like Counter(), defaultdict eases dictionary creation\n",
    "with open(emolex_file, 'r') as f:\n",
    "    # emolex file format is: word emotion value\n",
    "    for line in f:\n",
    "        word, emotion, value = line.strip().split()\n",
    "        nrc_lexicon[word][emotion] = int(value)\n",
    "        \n",
    "# Print a sample of the NRC EmoLex lexicon\n",
    "print('\\nNRC lexicon sample')       \n",
    "for key in random.sample(tuple(nrc_lexicon.keys()), 2):\n",
    "    print(f'{key}:', nrc_lexicon[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define scoring function\n",
    "\n",
    "Set up a function to score a word as positive or negative using either the NLTK or NRC dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def word_sentiment_score(word, method='nrc', lex=nrc_lexicon):\n",
    "    '''\n",
    "    Takes a word, optional method in ['nrc', 'nltk'], and optional lexicon dictionary.\n",
    "    Returns 1 (if positive), -1 (if negative), 0 (neutral), or None (not in lex).\n",
    "    '''\n",
    "    word = word.lower() # Handle non-case-folded inputs\n",
    "    if method.lower() == 'nrc':\n",
    "        if word in lex: # Only score words that are in the lexicon\n",
    "            pos = lex[word]['positive']\n",
    "            neg = lex[word]['negative']\n",
    "            if pos == neg: # Ties (mostly 0==0) return zero\n",
    "                return 0\n",
    "            elif pos > neg:\n",
    "                return 1\n",
    "            else:\n",
    "                return -1\n",
    "    elif method.lower() == 'nltk':\n",
    "        if word in lex['positive']:\n",
    "            return 1\n",
    "        elif word in lex['negative']:\n",
    "            return -1\n",
    "    else:\n",
    "        raise NameError(\"Method not in ['nrc', 'nltk']\")\n",
    "    return None # If word not in lexicon, return None (not zero). Why do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"NRC  'analyst':\", word_sentiment_score('analyst'))\n",
    "print(\"NLTK 'analyst':\", word_sentiment_score('analyst', method='nltk', lex=nltk_lexicon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score our example sentences\n",
    "\n",
    "For reference, here are our class-crowd-sourced scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Human scores\n",
    "print(\"Method: human\")\n",
    "print(\"Sentence scores:\", human_scores)\n",
    "print(\"Summary score:\", sum(human_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate and compare dictionary scores\n",
    "\n",
    "# Scoring methods for input\n",
    "methods = {\n",
    "    'nltk':nltk_lexicon,\n",
    "    'nrc' :nrc_lexicon\n",
    "}\n",
    "\n",
    "# Calculate, record, and print scores\n",
    "method_scores = {} # To store results\n",
    "for method in methods:\n",
    "    sentence_scores = [] # Could rewrite next few lines as a list comprehension\n",
    "    for sent in tokens:\n",
    "        sentence_score = 0\n",
    "        for word in sent:\n",
    "            word_score = word_sentiment_score(word, method=method, lex=methods[method])\n",
    "            if word_score != None:\n",
    "                sentence_score += word_score\n",
    "        sentence_scores.append(sentence_score)\n",
    "    method_scores[method] = sentence_scores # Save sentence-level scores\n",
    "    # Print results\n",
    "    print(\"Method:\", method)\n",
    "    print(\"Sentence scores:\", sentence_scores)\n",
    "    print(\"Summary score:\", sum(sentence_scores),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize and print the story for discussion\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = list(range(len(tokens)))\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plt.plot(x, human_scores, '-',  c='black', label='human', alpha=0.7, linewidth=3)\n",
    "plt.plot(x, method_scores['nltk'], '--', c='blue', label='nltk', alpha=0.7, linewidth=3)\n",
    "plt.plot(x, method_scores['nrc'], '-.', c='red', label='nrc', alpha=0.7, linewidth=3)\n",
    "plt.legend()\n",
    "plt.title(\"Sentiment scores\")\n",
    "plt.show()\n",
    "\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss \n",
    "\n",
    "Do these scores make sense? \n",
    "\n",
    "* We can look at specific instances, like NRC on the first sentence (next code block)\n",
    "* **Is this a happy story or not?**\n",
    " * Do the summary scores reflect our judgment about that?\n",
    " * If not, why not and how could we improve those scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Token-level NRC scores for example sentence 1\n",
    "for word in tokens[0]:\n",
    "    print(f'{word.ljust(11)}{word_sentiment_score(word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Seaborn to plot data with lowess (local regression) fit\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_context('talk')\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.regplot(x=x, y=human_scores, lowess=True, color='k', label='human')\n",
    "sns.regplot(x=x, y=method_scores['nrc'], lowess=True, color='r', label='nrc')\n",
    "sns.regplot(x=x, y=method_scores['nltk'], lowess=True, color='b', label='nltk')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Madame Bovary* (Flaubert, 1856/1857)\n",
    "\n",
    "Expected arc: starts happy, ends sad.\n",
    "\n",
    "Note that we do not lowercase our text (why not?), nor do we remove stopwords and punctuation (again, why not?). We *could* do both of those things, but would want to be sure that there weren't any name collisions in our text (that is, names that have affective associations when used as common words). We might also want to preserve case and punctuation for other tasks in our processing pipeline, even if we don't need them for our sentiment score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and tokenize the novel\n",
    "bovary_file = os.path.join('..', 'data', 'texts', 'F-Flaubert-Madame_Bovary-1857-M.txt')\n",
    "with open(bovary_file, 'r') as f:\n",
    "    bovary_text = f.read()\n",
    "bovary = [word_tokenize(sent) for sent in sent_tokenize(bovary_text)]\n",
    "print(\"Sentences:\", len(bovary))\n",
    "print(\"Total tokens:\", sum([len(sent) for sent in bovary]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note one small change below: we divide sentence sentiment by sentence length, so that long sentences don't count more than short ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score it using NLTK and NRC methods\n",
    "bovary_scores = {}\n",
    "for method in methods:\n",
    "    sentence_scores = []\n",
    "    for sent in bovary:\n",
    "        sentence_score = 0\n",
    "        for word in sent:\n",
    "            word_score = word_sentiment_score(word, method=method, lex=methods[method])\n",
    "            if word_score != None:\n",
    "                sentence_score += word_score\n",
    "        sentence_scores.append(sentence_score/len(sent))\n",
    "    bovary_scores[method] = sentence_scores\n",
    "    print(\"Method:\", method)\n",
    "    print(\"Summary score:\", round(sum(bovary_scores[method]),2),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results with 4th-order polynomial fit (why?)\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "x_bov = [i/len(bovary) for i in range(len(bovary))] # normalize length as fraction of all sentences\n",
    "sns.regplot(x=x_bov, y=bovary_scores['nltk'], order=4, scatter=False, color='b', label=None)\n",
    "sns.regplot(x=x_bov, y=bovary_scores['nltk'], x_bins=20, scatter=True, fit_reg=False, color='b', label='nltk')\n",
    "sns.regplot(x=x_bov, y=bovary_scores['nrc'], order=4, scatter=False, color='r', label=None)\n",
    "sns.regplot(x=x_bov, y=bovary_scores['nrc'], x_bins=20, scatter=True, fit_reg=False, color='r', label='nrc')\n",
    "plt.title(\"Sentiment in $\\it{Madame\\ Bovary}$\")\n",
    "plt.xlabel(\"Narrative time\")\n",
    "plt.ylabel(\"Average binned sentiment\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss:** How would you compare and evaluate the results of these two methods on this text?\n",
    "\n",
    "## Where to go from here\n",
    "\n",
    "* More validation!\n",
    "  * Is your lexicon good? For this kind of text? Written at this time?\n",
    "* Other dictionaries\n",
    "  * Develop your own?\n",
    "  * See Jurafsky and Martin for clever and/or complex ideas\n",
    "    * Example: tag adjectives. Those that appear on either side of the token `and` probably have the same sentiment polarity; those linked by `but` are likely opposites.\n",
    "    * Lots of embedding-based approaches, too.\n",
    "* Other aspects of sentiment/emotion/affect \n",
    "  * NRC 'anger', 'surprise', 'trust', etc.\n",
    "* Other texts and other *types* of text\n",
    "* Combine sentiment with other kinds of text- and sentence-level scoring\n",
    "  * Gender, time period (linguistic drift, yikes!), translations of the same text, news coverage of candidates, ...\n",
    "* More ideas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
