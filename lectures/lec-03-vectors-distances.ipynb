{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 03: Vectorization and distance metrics\n",
    "\n",
    "## To do\n",
    "\n",
    "* Friday sections\n",
    "  * Be prepared to discuss the readings in depth (Ramsay, Healy, Rambsy)\n",
    "  * PS1 to be released (due in c. two weeks, Thurs. 9/14, 11:59pm)\n",
    "* PS0 (shakedown) due tomorrow, 11:59pm\n",
    "* No lecture next Monday (Labor Day)\n",
    "* Extra credit for good, consistent answers on Ed\n",
    "* Study groups are great for homeworks.\n",
    "* Questions?\n",
    "\n",
    "## Definitions\n",
    "\n",
    "* What is a **vector**?\n",
    "  * An ordered collection of numbers that locate a point in space relative to a shared reference point (called the *origin*).\n",
    "  * We can also think of vectors as representing the quantified *features* of an object.\n",
    "  * Vectors are usually written as *row matrices*, or just as lists: $vec = [1.0, 0.5, 3.0, 1.2]$\n",
    "  * Vectors have as many *dimensions* as there are features of the object to represent.\n",
    "    * The number of features to represent is a choice of the experiment. There is no correct choice, though some choices are better than others for a given purpose.\n",
    "* What is **vectorization**?\n",
    "  * The process of transforming an object into its vector representation, typically by measuring some of the object's properties.\n",
    "  \n",
    "## Why would we want to do this?\n",
    "\n",
    "One goal of humanistic inquiry and of scientific research is to compare objects, so that we can gather them into types and compare any one object to others that we observe. Think of biological species or literary genres or historical eras. But how can we measure the difference or similarity between objects that are, after all, always necessarily individual and unique?\n",
    "\n",
    "* Measuring the *properties* of objects lets us compare those objects to one another.\n",
    "  * But ... *which* properties?\n",
    "  * Example: We might count words by type to compare gender and sentiment in novels.\n",
    "* Establishing a vector representation allows us to define a **distance metric** between objects that aren't straightforwardly spatial.\n",
    "  * \"Distance\" is a metaphor. Ditto \"similarity.\"\n",
    "  * Nothing is, in itself, like or unlike anything else. \n",
    "    * We sometimes seek to assert that objects are similar by erasing aspects of their particularity.\n",
    "  * Measuring similarity and difference are (always and only) interpretive interventions.\n",
    "  \n",
    "## A spatial example\n",
    "\n",
    "Consider this map of central campus:\n",
    "\n",
    "![](images/cornell_map.png)\n",
    "\n",
    "**How far apart are Gates Hall (purple star) and the clock tower (orange star)?**\n",
    "\n",
    "What do we need to know or define in order to answer this question?\n",
    "\n",
    "* Where is each building in physical space.\n",
    "  * Latitude/longitude; meters north/south and east/west of the book store; etc.\n",
    "* How do we want to measure the distance between them (walking, driving, flying, tunneling, ...). Minutes or miles?\n",
    "\n",
    "Normal, boring answer: about 0.4 miles on foot via Campus Rd and Ho Plaza, or a bit less if you cut some corners, or less than 0.3 miles if you can fly.\n",
    "\n",
    "| Clock tower | Gates Hall |\n",
    "| --- | --- | \n",
    "| ![](images/clock_tower.jpg) | ![](images/gates.jpg) |\n",
    "\n",
    "More interesting version: How far apart are these buildings conceptually? Architecturally? Historically? \n",
    "\n",
    "* What are the features and metrics you would use to answer this question?\n",
    "* This is a lot more like the problem of comparing texts.\n",
    "\n",
    "## A textual example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = '''\\\n",
    "My cat likes water.\n",
    "The dog eats food.\n",
    "The dog and the cat play together.\n",
    "A dog and a cat meet another dog and cat.\n",
    "The end.'''\n",
    "\n",
    "# Print with sentence numbers\n",
    "for line in enumerate(text.split('\\n')):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us stipulate that we want to compare these five sentences according to their \"*dogness*\" and \"*catness*.\" We care about those two aspects alone, nothing else.\n",
    "\n",
    "Let's develop some intuitions here:\n",
    "\n",
    "* Sentences 0 and 1 are as far apart as can be: 0 is about cats, 1 is about dogs.\n",
    "* Sentence 2 lies between 0 and 1. It contains a mix of dogness and catness.\n",
    "* Sentence 3 is kind of like sentence 2, but it has twice as much of both dogness and catness.\n",
    "  * How different are sentences 2 and 3? (There's no objectively correct answer.)\n",
    "* Sentence 4 is a zero point. It has no dogness or catness.\n",
    "\n",
    "### Count relevant words\n",
    "\n",
    "||**cat**|**dog**|\n",
    "|---|---|---|\n",
    "|**sentence**| | |\n",
    "|0|1|0|\n",
    "|1|0|1|\n",
    "|2|1|1|\n",
    "|3|2|2|\n",
    "|4|0|0|\n",
    "\n",
    "The **vector representation** of sentence 0 is `[1, 0]`. The vector representation of sentence 3 is `[2, 2]`. And so on ...\n",
    "### Visualize (scatter plot)\n",
    "\n",
    "Sketch this by hand ...\n",
    "\n",
    "### Distance measures\n",
    "\n",
    "How far apart are sentences 0 and 1 (and all the rest)?\n",
    "\n",
    "#### Manhattan distance\n",
    "\n",
    "* Also called \"city block\" distance. \n",
    "* Not much used, but easy to understand and to compute (which matters for very large data sets). \n",
    "* Sum of the absolute difference in each dimension.\n",
    "\n",
    "For **sentences 0 and 1**, the Manhattan distance = |1| + |-1| = 2.\n",
    "\n",
    "#### Euclidean distance\n",
    "\n",
    "* Straight-line or \"as the crow flies\" distance. \n",
    "* Widely used in data science, but not always the best choice for textual data.\n",
    "\n",
    "Recall the Pythagorean theorem for the hypotenuse of a triangle: $a^2 = b^2 + c^2$ or $a = \\sqrt{b^2 +c^2}$.\n",
    "\n",
    "For **sentences 0 and 1**, the Euclidean distance = $\\sqrt{1^2 + 1^2} = \\sqrt{2} = 1.414$.\n",
    "\n",
    "OK, but what about the Euclidean distance between **sentence 0 and sentence 3**? Well, that distance = $\\sqrt{1^2 + 2^2} = \\sqrt{5} = 2.24$.\n",
    "\n",
    "And between **sentences 2 and 3** (both balanced 50:50 between dogs and cats)? That's 1.4 again, the same as the distance between sentences 0 and 1 (which, recall, are totally divergent in dog/cat content).\n",
    "\n",
    "An obvious improvement in this case would be to **normalize word counts by document length**.\n",
    "\n",
    "#### Cosine distance\n",
    "\n",
    "Maybe instead of distance, we could measure the difference in **direction** from the origin between points.\n",
    "\n",
    "* **Sentences 0 and 1** are 90 degrees apart.\n",
    "* **Sentences 2 and 3** are 0 degrees apart.\n",
    "* **Sentences 0 and 1** are each 45 degrees away from **sentences 2 and 3**.\n",
    "\n",
    "Now, recall the values of the **cosine** of an angle between 0 and 90 degrees. (Sketch by hand)\n",
    "\n",
    "So, the cosines of the angles between sentences are:\n",
    "\n",
    "sentences|angle|cosine\n",
    "---|---|---\n",
    "0 and 1|90|0\n",
    "2 and 3|0|1\n",
    "0 and 2|45|0.707\n",
    "0 and 3|45|0.707\n",
    "1 and 2|45|0.707\n",
    "\n",
    "We could then transform these cosine **similarities** into **distances** by subtracting them from 1, so that the most *dissimilar* sentences (like 0 and 1) have the greatest distance between them.\n",
    "\n",
    "The big advantage here is that we don't need to worry about getting length normalization right. Cosine distance is often a good choice for text similarity tasks.\n",
    "\n",
    "#### Higher dimensions\n",
    "\n",
    "All of these metrics can be calculated in arbitrarily many dimensions. Which is good, because textual data is often very high-dimensional. Imagine counting the occurrences of each word type in a large corpus of novels or historical documents. Can easily be tens of thousands of dimensions.\n",
    "\n",
    "## In the real world\n",
    "\n",
    "* There's nothing wrong with any of these vectorizations and distance metrics, exactly, but they're not state of the art.\n",
    "* If you've done some recent NLP work, you'll know that, at the very least, you'd want to use static word embeddings in place of raw tokens.\n",
    "  * This allows you to capture the similarity of meaning between, e.g., \"cat\" and \"kitten.\"\n",
    "  * Word counts alone represent any two distinct word types as (entirely) separate dimensions, so \"cat\" and \"kitten\" have the same inherent relationship (none) as \"cat\" and \"dog\" or \"cat\" and \"algebraic\".\n",
    "* If you were especially ambitious, you'd be looking at something like BERT or ELMo or GPT-*, etc.\n",
    "    * These transformer-based methods allow for *contextual* embeddings, that is, they represent a word token differently depending on the context in which it appears, so that the representation of \"bank\" in \"my money is in the bank\" is different from the the representation of \"bank\" in \"we walked along the bank of the river.\"\n",
    "* We'll cover both static and contextual embeddings later this semester.\n",
    "* And then you might want features that correspond to aspects of a text other than the specific words it contains.\n",
    "    * When was it written?\n",
    "    * By *whom* was it written?\n",
    "    * How long is it?\n",
    "    * In what style is it written?\n",
    "    * Who read it?\n",
    "    * How much did it cost?\n",
    "    * How many people read or reviewed it?\n",
    "    * What else did its readers also read?\n",
    "    * And so on ...\n",
    "\n",
    "Here, though, we're trying to grasp the *idea* behind document similarity, on which all of these methods depend: transform text into a numeric representation of its features (often, a representation of its content or meaning), then quantify the difference or similarity between those numeric representations.\n",
    "\n",
    "## In the problem set world\n",
    "\n",
    "We'll dig into how, as a practical matter, we can vectorize texts and calclulate distance metrics in this week's problem set.\n",
    "\n",
    "We'll use `scikit-learn` to implement vectorization and distance metrics. The `scikit-learn` API almost always involves *three* steps:\n",
    "\n",
    "1. Instantiate a learning object (such as a vectorizer, regressor, classifier, etc.). This is the object that will hold the parameters of your fitted model.\n",
    "1. Call the instantiated learning object's `.fit()` method, passing in your data. This allows the model to learn the optimal parameters from your data.\n",
    "1. Call the fitted model's `.transform()` or `.predict()` method, passing in either the same data from the `fit` step or new data. This step uses the fitted model to generate outputs given the input data you supply.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# get example text as one doc per line\n",
    "docs = [sent for sent in text.split('\\n')]\n",
    "\n",
    "# instantiate vectorizer object\n",
    "#  note setup options\n",
    "vectorizer = CountVectorizer(\n",
    "    vocabulary=['cat', 'dog']\n",
    ")\n",
    "\n",
    "# fit to data\n",
    "vectorizer.fit(docs)\n",
    "\n",
    "# transform docs to features\n",
    "features = vectorizer.transform(docs)\n",
    "\n",
    "# print output feature matrix\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(features.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances, cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "print(\"Euclidean distances\")\n",
    "print(np.round(euclidean_distances(features),2))\n",
    "\n",
    "print(\"\\nCosine distances\")\n",
    "print(np.round(cosine_distances(features),2))\n",
    "\n",
    "print(\"\\nCosine **similarities**\")\n",
    "print(np.round(cosine_similarity(features),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FYI, two heatmap visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Euclidean distances\")\n",
    "\n",
    "# quick and dirty\n",
    "plt.imshow(euclidean_distances(features))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prettier\n",
    "sns.heatmap(\n",
    "    euclidean_distances(features),\n",
    "    annot=True,\n",
    "    square=True\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word on the waitlist ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
