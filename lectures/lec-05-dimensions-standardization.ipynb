{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 05: Feature standardization, normalization, and dimension reduction\n",
    "\n",
    "## To do\n",
    "\n",
    "* PS1 due Thursday, 11:59pm\n",
    "    * Be sure to execute your code!\n",
    "    * **No credit for unexecuted code**; we *might* run your code, but we often do not. If there's no output in your notebook, you won't receive credit, even if your code would have produced proper results.\n",
    "* Reading (Moretti) for Wednesday/Friday\n",
    "    * Response due by 4:00pm Tuesday if assigned to you by NetID (Q-Z).\n",
    "* Friday: Section as usual\n",
    "    * Be prepared to discuss Moretti, even if you weren't assigned to write a response post. All readings are always required.\n",
    "* Next week (classification, supervised learning)\n",
    "    * Monday: No new reading\n",
    "    * Wednesday: Read articles by Allison et al. and by Mauch et al.\n",
    "        * Response due by 4:00pm next Tuesday if assigned to you by NetID (H-P)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Normalization and standardization\n",
    "\n",
    "First, note that the terms \"normalization\" and \"standardization\" are sometimes used interchangably or inconsistently. So be sure you understand what's going on in any given instance if you encounter there terms in the wild. In this class, we'll try to be consistent with the definitions below.\n",
    "\n",
    "### Normalization (`l1`, `l2`)\n",
    "\n",
    "* Normalization is the process by which we ensure that each vector in our feature matrix has the same length\n",
    "    * Normalization works *row-wise*, not *column-wise*. That is, we normalize the features attached to each *observation*.\n",
    "* The details vary, depending on our distance (length) metric\n",
    "* The two most commonly used normalizations are `l1` and `l2`\n",
    "* `l1` normalization adjusts the features so that they sum to one in each vector.\n",
    "    * This means that the *Manhattan length* of each vector is one\n",
    "    * It also means that the normalized value of each feature represents the fraction of the total feature counts/weight accounted for by that feature in a given document\n",
    "    * As previously discussed, `l1` norms preserve the original ratios between the features. This is often good for accuracy on downstream tasks.\n",
    "* `l2` normalization adjusts each vector so that the sum of the squared features is one\n",
    "    * This means that the *Euclidean length* of each vector is one\n",
    "    * `l2` norms decrease the effective weight of low-frequency features (hence, they increase the relative weight of high-frequency features). This can be good for interpretation, because it means that downstream tasks rely on a comparatively sparse set of important features.\n",
    "\n",
    "\n",
    "### Standardization (*z*-scores)\n",
    "\n",
    "* Even when we normalize our vectors, distances and similarities are dominated by common terms\n",
    "    * Common terms contribute most of the weight to the overall vector\n",
    "    * This might be what we want ...\n",
    "    * ... or it might not.\n",
    "* What if we care about the comparative usage rates of each included feature (word)?\n",
    "    * That is, what if every word should contribute equally?\n",
    "    * We could scale between, say, 0 and 1\n",
    "        * But then we're at the mercy of high and low outliers\n",
    "    * Instead, we often scale to mean zero and standard deviation one.\n",
    "        * This is called a \"standard score\" or \"*z*-score.\"\n",
    "* Standardization works *column-wise*. That is, it makes every *feature*, across all observations, as important as every other feature.\n",
    "\n",
    "Typically, you'll normalize first, then scale. But some situations call for just one or the other, or are not especially sensitive to either one. You might *not* want to normalize or scale if your feature data share a common (and meaningful) scale and are already normally distributed. Leaving your data alone in that case may not make much difference for your task performance and may make it easier to interpret your results.\n",
    "\n",
    "# An example\n",
    "\n",
    "Consider the following example of normalization and standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from   sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from   sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "\n",
    "# some invented data\n",
    "sample = [\n",
    "    [10,11,11,12],\n",
    "    [0,0,1,1],\n",
    "    [0,5,7,10]\n",
    "]\n",
    "df = pd.DataFrame(sample).T\n",
    "df.columns = ['the', 'cat', 'she']\n",
    "\n",
    "print('raw features:')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and distances in raw invented data\n",
    "print(\"raw mean:\", round(np.mean(df.to_numpy()),2))\n",
    "print('raw distances:')\n",
    "display(pd.DataFrame(euclidean_distances(df)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine effects of scaling and norming\n",
    "print('\\n\\nmin-max scaling:')\n",
    "minmax = MinMaxScaler().fit_transform(df)\n",
    "display(minmax)\n",
    "print(\"min-max mean:\", round(np.mean(minmax),3))\n",
    "print('min-max distances:')\n",
    "display(pd.DataFrame(euclidean_distances(minmax)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n\\nz-scores:')\n",
    "zscores = StandardScaler().fit_transform(df)\n",
    "display(zscores)\n",
    "print('z-score mean:', round(np.mean(zscores),3))\n",
    "print('z-score distances:')\n",
    "display(pd.DataFrame(euclidean_distances(zscores)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n=====\\nl1 norm:')\n",
    "l1 = normalize(df, norm='l1')\n",
    "display(l1)\n",
    "print('l1 mean:', round(np.mean(l1),3))\n",
    "print('l1 distances:')\n",
    "display(pd.DataFrame(euclidean_distances(l1)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n\\nl2 norm:')\n",
    "l2 = normalize(df, norm='l2')\n",
    "display(l2)\n",
    "print('l2 mean:', round(np.mean(l2),3))\n",
    "print('l2 distances:')\n",
    "display(pd.DataFrame(euclidean_distances(l2)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))\n",
    "\n",
    "print('\\n=====\\nz-scored l2 norm:')\n",
    "zl2 = StandardScaler().fit_transform(l2)\n",
    "display(zl2)\n",
    "print('z-scored l2 mean:', round(np.mean(zl2),3))\n",
    "print('z-scored l2 distances:')\n",
    "display(pd.DataFrame(euclidean_distances(zl2)).mask(lambda x: x==0,np.nan).style.background_gradient(cmap='RdYlGn', axis=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a single plot with all six feature (not distance) matrices\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,8))\n",
    "for i, mat in enumerate([df, minmax, zscores, l1, l2, zl2]):\n",
    "    plt.subplot(int(f'23{i+1}'))\n",
    "    plt.imshow(mat)\n",
    "    plt.title(i)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "**You will often want to standardize your feature data, probably with *z*-scores.** \n",
    "\n",
    "## Dimension reduction\n",
    "\n",
    "### The curse of dimensionality\n",
    "\n",
    "Textual data are prone to high dimensionality, at least when your features are token counts. The vocabulary of a language (and generally of a corpus of documents written in that language) is large. If every token type is a feature, it's easy to have 10,000+ features in even a smallish corpus.\n",
    "\n",
    "High feature dimensionality relative to the number of observations (books or documents, for example) in your data leads to the \"[curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality).\"\n",
    "\n",
    "* Low density of samples\n",
    "    * Hard to identify \"typical\" or \"average\" points. \n",
    "    * Everything is an outlier.\n",
    "    * All points are far apart (or have low similarity, or are uncorrelated).\n",
    "* Multicolinearity\n",
    "    * Always true when you have more dimensions than samples.\n",
    "    * Many variables might be substituted for one another.\n",
    "    * But *which ones*?\n",
    "    * This is a problem if we want to *interpret* our model.\n",
    "* Overfitting\n",
    "    * Too much \"detail\" in our training data.\n",
    "    * For example, say we care about cats in our texts.\n",
    "        * Do we need features `['cat', 'cats', 'kitten', 'kittens', 'kitty', 'kitties', 'Cat', 'Cats', ...]`?\n",
    "        * Probably not; any one of these, or their sum, would do.\n",
    "        \n",
    "#### High-dimensionality demo\n",
    "\n",
    "Let's look at the distances between an arbitrary number of points in spaces of increasing dimensionality ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def distance_distribution(dims=2, points=100, distance_function=euclidean_distances):\n",
    "    '''Find median distance between points in n-dimensional space and\n",
    "       plot histogram of all distances'''\n",
    "    data = np.random.rand(points,dims)\n",
    "    distances = distance_function(data)\n",
    "    np.fill_diagonal(distances, np.nan)\n",
    "    print(f\"Median distance ({points} points in {dims} dimensions): {np.nanmedian(distances):7.4f}\")\n",
    "    plt.hist(np.ravel(distances), bins=20)\n",
    "    plt.xlabel(\"distance\")\n",
    "    plt.ylabel(\"frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "    return distances\n",
    "\n",
    "#print(distance_distribution.toarray())\n",
    "_ = distance_distribution(dims=2, points=1000, distance_function=cosine_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "high_dim_distances = distance_distribution(dims=500, points=1000, distance_function=cosine_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what angle corresponds to the median distance?\n",
    "print(f\"Median angle: {np.arccos(1-np.nanmedian(high_dim_distances))*180/np.pi:7.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The trick, when faced with high-dimensional data, is to figure out *which* features to keep (feature selection) and/or how collapse multiple features into one (dimension reduction).\n",
    "\n",
    "## Feature selection\n",
    "\n",
    "We'll talk more about this later, but for now, a few key points:\n",
    "\n",
    "* If a feature has the same value for most objects (that is, it has low variance), it is unlikely to be informative and is a good candidate for elimination.\n",
    "* We're looking to hold on to as much of the underlying variance (information) in the data as possible, while eliminating as many features as possible.\n",
    "    * Any measure of correlation or mutual information would help us identify features that provide similar information.\n",
    "    * We might then drop one or more of those variables with little loss of overall information.\n",
    "* We can also work empirically and *post hoc* by calculating feature importances from our classifier (where possible).\n",
    "    * We then retain only the *n* most important features and examine the impact on classifier performance.\n",
    "\n",
    "\n",
    "## Linear and manifold methods\n",
    "\n",
    "But we can also *transform* our features, rather than just dropping some and retaining others.\n",
    "\n",
    "Specifically, we can look for mathematical *combinations* of features that hold on to all or most of the underlying variance.\n",
    "\n",
    "Consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearly related variables\n",
    "x = np.linspace(0,5)\n",
    "y = x\n",
    "\n",
    "plt.scatter(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we know *x*, we already know the *exact* value of *y*!\n",
    "* Here, we could just drop *x* or *y*.\n",
    "\n",
    "But what about this case?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# NB. make_blobs creates synthetic data for demo purposes.\n",
    "# You do not use make_blobs as part of a real analysis workflow.\n",
    "X, y = make_blobs(n_samples=1000, centers=1)\n",
    "transformation = [[-0.6, -0.6], [0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "plt.scatter(X_aniso[:,0], X_aniso[:,1], alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# A function to draw vectors\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# Set up and fit PCA\n",
    "pca = PCA(n_components=2, whiten=True) # whiten enforces some extra regularity on output\n",
    "X_pca = pca.fit_transform(X_aniso)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(3,1, figsize=(4.5, 12))\n",
    "\n",
    "# Input data\n",
    "ax[0].scatter(X_aniso[:, 0], X_aniso[:, 1], alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v, ax=ax[0])\n",
    "ax[0].axis('equal');\n",
    "ax[0].set(xlabel='x', ylabel='y', title='Input')\n",
    "\n",
    "# PCA 2-D\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.2)\n",
    "draw_vector([0, 0], [0, 3], ax=ax[1])\n",
    "draw_vector([0, 0], [3, 0], ax=ax[1])\n",
    "ax[1].axis('equal')\n",
    "ax[1].set(xlabel='PC0', ylabel='PC1',\n",
    "          title='PCs')\n",
    "\n",
    "# PCA 1-D\n",
    "ax[2].scatter(X_pca[:,0], np.zeros(shape=len(X_aniso)), alpha=0.1)\n",
    "ax[2].set(xlabel='PC0', ylabel='None', title='1-D')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance:\", pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated SVD\n",
    "\n",
    "Singular Value Decomposition (SVD) is closely related to PCA. PCA is just SVD performed after z-scoring the input data. The only difference, from our perspective, is that, because PCA needs to standardize input data, it requires dense (rather than sparse) input (and produces dense output). So, we use SVD (via `TrunctedSVD`) when we want to preserve input sparsity (e.g., when our dataset is very large). \"Truncated\" just means that we retain fewer dimensions in our output than existed in the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=1)\n",
    "X = svd.fit_transform(X_aniso)\n",
    "plt.scatter(X[:,0], np.zeros(shape=len(X)), alpha=0.1)\n",
    "plt.show()\n",
    "print('Explained variance:', round(svd.explained_variance_ratio_[0],4))\n",
    "svd.fit_transform(StandardScaler().fit_transform(X_aniso))\n",
    "print('Explained variance using standardized data:', round(svd.explained_variance_ratio_[0],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "* Standardizing our input data captures about the same amount of variance as does PCA. It's slightly different because we used the `whiten` option with PCA, which assures unit variance in each each reduced output dimension at the typical cost of a small amount of lost variance.\n",
    "* We've used a one-dimensional plot here *not* because SVD is doing something different from PCA, but to show what dimension *reduction* looks like. We had 2-D inputs; it doesn't really make sense to use 2-D outputs!\n",
    "\n",
    "### *t*-SNE\n",
    "\n",
    "*t*-distributed Stochastic Neighbor Embedding is a *manifold* method. Features are projected into a multidimensional manifold rather than onto lines.\n",
    "\n",
    "TSNE is (or was; see below) widely used for visualization, because it's good at maintaining internal structure (\"lumpiness\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(init='random', learning_rate='auto')\n",
    "X = tsne.fit_transform(X_aniso)\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "Uniform Manifold Approximation and Projection (UMAP) is a manifold method, like *t*-SNE. It's more computationally efficient than *t*-SNE and tends to perform a bit better, too (in the sense that it preserves more of the underlying density structure. UMAP is generally preferred to *t*-SNE for visualization these days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "umap_reducer = umap.UMAP()\n",
    "X = umap_reducer.fit_transform(X_aniso)\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of parameters that control the performance of UMAP (and of *t*-SNE, too). If you make any real use of UMAP, you should read (and understand) [the documentation](https://umap-learn.readthedocs.io/en/latest/parameters.html#)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
