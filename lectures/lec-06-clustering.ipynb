{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 06: Clustering\n",
    "\n",
    "## To do\n",
    "\n",
    "* Week 5 reading\n",
    "    * Wednesday: Read articles by Allison et al. and by Mauch et al.\n",
    "        * Response due by 4:00pm Tuesday if assigned to you by NetID (H-P)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised and unsupervised learning\n",
    "\n",
    "* \"Clustering\" is a type of *unsupervised* learning\n",
    "* \"Classification\" (next week) is a type of *supervised* learning\n",
    "* Both seek to assign a finite set of labels to input objects on the basis of features associated with those objects\n",
    "* In *supervised* learning, you know the labels in advance\n",
    "    * You supply a set of (correctly) labeled objects to the algorithm\n",
    "    * The algorithm then \"learns\" which features are associated with which labels, so as to minimize classification errors\n",
    "* In *unsupervised* learning, you don't know the labels (though you might know -- or at least have a sense about -- how *many* labels there should be)\n",
    "    * You supply a feature matrix and a definition of what a \"good\" clustering should be\n",
    "    * The algorithm then assigns labels to the input objects so as to best satisfy the supplied definition of \"good\" (typically, minimizing variance within clusters and maximizing difference between clusters)\n",
    "\n",
    "## Why prefer one or the other?\n",
    "\n",
    "* Unsupervised methods are often used early in a project, when you're looking for unknown stucture in your data\n",
    "    * Unsupervised methods are your only option if you don't know what the appropriate set of labels might be for your data set\n",
    "    * Also appropriate if you don't have (many) labeled instances\n",
    "    * Unsupervised methods are typically \"cheap\" to set up, but costly to evaluate\n",
    "* Supervised methods require you to know in advance the full set of appropriate labels for your data\n",
    "    * Supervised methods often have high initial costs, but are easier to evaluate (because you already have a set of correctly labeled instances that you can use for validation).\n",
    "\n",
    "## Cluster boundaries\n",
    "\n",
    "[sketch the problem]\n",
    "\n",
    "* Note that not all clusters have the same general shape\n",
    "* Spherical blobs, separate linear blobs, areas of varying density, etc.\n",
    "* There is no universally best way to draw decision boundaries\n",
    "\n",
    "## *k*-means clustering\n",
    "\n",
    "The algorithm:\n",
    "\n",
    "1. Select *k* objects from the data set to serve as initial cluster centers (\"centroids\")\n",
    "1. Assign each object in the data set to the nearest centroid. These are your initial clusters.\n",
    "1. Calculate the mean location of the objects assigned to each cluster. These are your new centroids.\n",
    "1. If the new centroids are sufficiently close to the previous centroids, you're done.\n",
    "    1. If the new centroids are not sufficiently close to the old centroids, use the new centroids as the basis for a new clustering.\n",
    "    1. Repeat labeling, centroid calculation, and difference comparison until centroids are stable (enough).\n",
    "\n",
    "## Other clustering methods\n",
    "\n",
    "* Ward (hierarchical), agglomerative\n",
    "* Density-based (DBSCAN, OPTICS)\n",
    "* Graph distance (affinity propagation, spectral)\n",
    "* ...\n",
    "\n",
    "## An artificial example\n",
    "\n",
    "### *k*-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k-Means on articial data\n",
    "# Adapted from sklearn examples\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "# make artificial data (demo only, not part of the real-world process)\n",
    "n_samples = 1500\n",
    "random_state = 42\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "# Unlabeled data\n",
    "plt.subplot(231)\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.title(\"Unclustered data\")\n",
    "\n",
    "# Correct number of clusters\n",
    "#  Note we are doing three steps (instantiate, fit, predict)\n",
    "#  in one line of code here\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state, n_init='auto').fit_predict(X)\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"k=3\")\n",
    "\n",
    "\n",
    "# Incorrect number of clusters\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state, n_init='auto').fit_predict(X)\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "plt.title(\"k=2\")\n",
    "\n",
    "# Different variance\n",
    "X_varied, y_varied = make_blobs(n_samples=n_samples,\n",
    "                                cluster_std=[1.0, 4.0, 1.0],\n",
    "                                random_state=random_state)\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1])\n",
    "plt.title(\"Unequal density\")\n",
    "\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state, n_init='auto').fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"k=3\")\n",
    "\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state, n_init='auto').fit_predict(X_varied)\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], c=y_pred)\n",
    "plt.title(\"k=2\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTICS\n",
    "\n",
    "Another method implemented by `scikit-learn`, so easy to plug in. See the [package documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html) and the [user guide](https://scikit-learn.org/stable/modules/clustering.html#optics). \n",
    "\n",
    "OPTICS is a density-based algorthim similar to DBSCAN or (especially) to HDBSCAN (since the later, like OPTICS, allows variable cut points by calculating an ordered reachability graph between points). The advantage of OPTICS and HDBSCAN over base DBSCAN is the ability to identify clusters of different densities. But this also means that parameters like `xi` (OPTICS) and `min_samples` (HDBSCAN), as well as `eps` in original DBSCAN are really important. The defaults are sane for normalized, scaled, normally distributed data, but they might not be right for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "# perform clustering\n",
    "# xi defines the cutoff for steepness of the reachability curve\n",
    "optics_clusterer = OPTICS(min_samples=50, xi=0.05, min_cluster_size=0.01)\n",
    "optics_clusterer.fit(X_varied)\n",
    "y_optics = optics_clusterer.labels_\n",
    "\n",
    "# plot result\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.scatter(X_varied[:, 0], X_varied[:, 1], alpha=0.5, linewidths=0)\n",
    "plt.title(\"Unclustered\")\n",
    "\n",
    "plt.subplot(122)\n",
    "\n",
    "plt.scatter(X_varied[y_optics >=0, 0], X_varied[y_optics >=0, 1], c=y_optics[y_optics>=0], alpha=0.5, linewidths=0)\n",
    "plt.scatter(X_varied[y_optics <0, 0], X_varied[y_optics <0, 1], c='k', marker='.', alpha=0.5, linewidths=0)\n",
    "plt.title(\"OPTICS clustering\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo: Historical cookbooks\n",
    "\n",
    "We have a dataset of almost 50,000 recipes from 70+ American cookbooks (sourced from the [Feeding America](https://d.lib.msu.edu/fa) project at Michigan State, processed via [Humanities Data Analysis](https://www.humanitiesdataanalysis.org/introduction-cook-books/notebook.html#an-exploratory-data-analysis-of-the-united-states-culinary-history)). Let's see if we can discover some structure in this dataset.\n",
    "\n",
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read the data\n",
    "import os\n",
    "import pandas as pd\n",
    "cook = pd.read_csv(os.path.join('..', 'data', 'cookbooks', 'feeding-america.csv'))\n",
    "cook.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how many recipes of each type?\n",
    "cook.recipe_class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# subset the data to bread recipes and meat recipes\n",
    "breadmeat = cook.loc[cook.recipe_class.isin([\"breadsweets\", \"meatfishgame\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    strip_accents='unicode',\n",
    "    lowercase=True,\n",
    "    use_idf=False,\n",
    "    tokenizer=lambda x: x.split(';'), # use existing tokenization by ingredient\n",
    "    token_pattern=None,\n",
    "    min_df=100 # limit to common ingedients\n",
    ")\n",
    "features = vectorizer.fit_transform(breadmeat['ingredients'])\n",
    "print(\"Feature matrix shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# glance at some feature names\n",
    "vectorizer.get_feature_names_out()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize the data via PCA/SVD\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import seaborn as sns\n",
    "\n",
    "X_reduced = TruncatedSVD().fit_transform(features)\n",
    "sns.scatterplot(x=X_reduced[:,0], y=X_reduced[:,1], hue=breadmeat['recipe_class'], alpha=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FYI, these \"wedge-shaped\" PCA plots are common with text data. Waving hands a bit, the edges of the wedge correspond to the edges of the upper right quadrant in the original data (that is, our values for every feature are strictly non-negative); the density of data near the point of the wedge indicates that there are lots of zero values in our data (which is typical of text data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vis with UMAP\n",
    "from umap import UMAP\n",
    "\n",
    "X_umap = UMAP().fit_transform(features)\n",
    "sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=breadmeat['recipe_class'], alpha=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster with KMeans\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state, n_init='auto').fit_predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=y_pred, hue_order=[1,0], alpha=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Examine cases where our cluster label doesn't match the existing label. This is fine as a pedagogical exercise, but remember that you would only really use unsupervised methods when you **don't** know the \"correct\" labels for your data. For that reason. we're not going to spend much time considering accuracy here. You **do** need to evaluate your clustering results, but you can't typically do it via direct accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"errors\"\n",
    "errors = breadmeat.loc[\n",
    "    ((breadmeat['recipe_class']=='meatfishgame') & (y_pred==0)) |\n",
    "    ((breadmeat['recipe_class']=='breadsweets')  & (y_pred==1))\n",
    "]\n",
    "print(\"Label differences:\", len(errors))\n",
    "errors.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Agglomerative clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n",
    "\n",
    "This one is slow ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clusterer = AgglomerativeClustering()\n",
    "y_pred = clusterer.fit_predict(features.toarray())\n",
    "\n",
    "sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=y_pred, hue_order=[1,0], alpha=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"errors\"\n",
    "errors = breadmeat.loc[\n",
    "    ((breadmeat['recipe_class']=='meatfishgame') & (y_pred==0)) |\n",
    "    ((breadmeat['recipe_class']=='breadsweets')  & (y_pred==1))\n",
    "]\n",
    "print(\"Label differences:\", len(errors))\n",
    "errors.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [DBSCAN](https://scikit-learn.org/stable/modules/clustering.html#dbscan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "clusterer = DBSCAN(min_samples=50, eps=0.75)\n",
    "y_pred = clusterer.fit_predict(features.toarray())\n",
    "\n",
    "sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], hue=y_pred, alpha=0.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
