{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 08: Feature selection\n",
    "\n",
    "## To do\n",
    "\n",
    "* Problem set 2 due Thursday, 9/28, at 11:59pm\n",
    "* Reading for section\n",
    "    * Read articles by Underwood, by Yauney, and by Piper\n",
    "    * NetIDs A-G reponse on Canvas by Tuesday, 9/26, at 4:00pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "\n",
    "We've spoken before about **overfitting**. A model that is overfit has learned features of the training data that are unlikely to generalize to new, unseen data. As a rule of thumb, a model is very likely to be overfit if it uses on the order of as many features as the training data contains examples. The signature of an overfit model is good performance when evaluated against the training data, but poor performance when evaluated against held-out test data. \n",
    "\n",
    "Dimension reduction is one approach to addressing overfitting. Another is **feature selection**. When we select features to retain in our model, we're looking for ones that seem likely to provide useful information about the relationship between our objects and the class labels we seek to assign. Two useful approaches to feature selection are **ANOVA f-statistics** and **mutual information**.\n",
    "\n",
    "### *F*-statistic and ANOVA analysis\n",
    "\n",
    "We won't say much more about ANOVA analysis; you should have seen it in your stats class. It's conceptually similar to a *t*-test, in that it compares the difference in the means of two groups relative to the variance within each group. A large *F*-statistic means that two samples are significantly different. In our case, we're comparing the distribution of values within each feature to the distribution of true labels in the training data. Intuitively, a feature may be a good predictor of the class label if its distribution is similar to the distribution of the label, hence if the F-statistic between them is small. (But note that scikit-learn's `f_classif` method returns the inverse of the *F*-statistic, such that a large result indicates a likely *useful* feature.)\n",
    "\n",
    "The problem with the *F*-statistic is that it only captures linear, normally distributed relationships. The advantages are (1) that this assumption is often valid and (2), it's super easy to calculate, so is fast when run on large datasets.\n",
    "\n",
    "### Mutual information\n",
    "\n",
    "**Mutual information** (MI) is abstractly similar to correlation or covariance -- it's a measure of how much two variables change together. But MI doesn't assume a linear relationship between the variables and it's suitable for categorical data, so is often preferred for our purposes.\n",
    "\n",
    "If you want the math, in the case of categorical data (like we're using here with word counts and class labels), mutual information is calculated as:\n",
    "\n",
    "$$MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|}\\frac{|U_i\\cap V_j|}{N}\\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}$$\n",
    "\n",
    "Where $\\cap$ indicates the **insection** of two sets. $N$ is the total number of observations in each vector. $i$ and $j$ represent the possible values in each vector. And $|U_i|$ is the count of instances of value $i$ in vector $U$.\n",
    "\n",
    "If we have two vectors, $U = [1,0]$ and $V = [1,0]$, we know that our MI score should be high, since $U$ is identical to $V$. For an *n*-class problem, the maximum unnormalized MI is log(*n*). In our example, $N$ = 2 (two observations per vector) and $i$ and $j$ are both drawn from `[1,0]` (or `[0,1]`, it doesn't matter). Hence:\n",
    "\n",
    "$$MI(U,V) = \\frac{1}{2}\\log\\frac{2(1)}{1(1)} + 0 + 0 + \\frac{1}{2}\\log\\frac{2(1)}{1(1)} = 0.693147$$\n",
    "\n",
    "The zeros correspond in this example to misaligned labels, of which there are none, hence $|U_i\\cap V_j| = 0$ for all $i\\neq j$ (again, in this specific example of perfect label alignment, not in all cases).\n",
    "\n",
    "Or consider the slightly more interesting case where $U = [1,1,2]$ and $V = [1,2,2]$. In this case, we calculate:\n",
    "\n",
    "$$MI(U,V) = \\frac{1}{3}\\log\\frac{3(1)}{2(1)} + \\frac{1}{3}\\log\\frac{3(1)}{2(2)} + 0 + \\frac{1}{3}\\log\\frac{3(1)}{1(2)} = 0.174416$$\n",
    "\n",
    "Mutual information is often (but not always) reported on a *normalized* basis, in which the raw MI value is divided by log($N$) (or, technically, by the mean entropy of the two variables, but it amounts to the same thing when there are the same number of classes in each variable), the maximum possible MI value for the case. The normalized MI for the first example is 0.693147/log(2) = 1.0. In ythe second example, normalized MI = 0.174416/log(3) = 0.15876.\n",
    "\n",
    "See the [sklearn documentation for `mutual_info_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif, f_classif, SelectKBest\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = np.array([[1,1,1,1,0,0], [1,0,1,0,1,0]]).T\n",
    "y = np.array([1,1,1,0,0,0])\n",
    "f_stat, p_vals = f_classif(X,y)\n",
    "print(f_stat)\n",
    "print(p_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SelectKBest(k=1, score_func=f_classif).fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mutual_info_classif(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A small corpus\n",
    "\n",
    "About 125,000 news articles, distributed evenly across four categories (world, business, sports, and science/technology), each trimmed to contain **just the first sentence or two of the original article**. [Data source](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv), plus minor massaging into current format.\n",
    "\n",
    "Our task will be to predict the category of any news article using a Bayesian classifier, and to examine the impact of feature selection on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and clean news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data from disk and examine\n",
    "news = pd.read_csv(os.path.join('..', 'data', 'news', 'news_text.csv.gz'))\n",
    "news.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I bet those datelines will be a problem. Let's get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a function to get rid of datelines at the start of articles\n",
    "#  matches one or more hyphens or colons in first 40 chars,\n",
    "#  drops everything before that match (plus the match itself)\n",
    "pattern = '[-:]+ '\n",
    "matcher = re.compile(pattern) # compiled regexs are faster\n",
    "\n",
    "def remove_dateline(text, matcher=matcher):\n",
    "    '''\n",
    "    Remove source names and datelines from a text string\n",
    "    If there is a hyphen or colon in the first 40 characters, \n",
    "      drops everything before the hyphen(s)/colon(s)\n",
    "    If no hyphen/colon, do nothing\n",
    "    Return processed string\n",
    "    '''\n",
    "    result = matcher.search(text, endpos=40)\n",
    "    if result:\n",
    "        return text[result.end():]\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clean article text\n",
    "news['body'] = news['body'].apply(remove_dateline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# confirm articles are cleaned\n",
    "news.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up a vectorizer object\n",
    "count_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode', # collapse accents to base character\n",
    "    stop_words=None, # do not remove stop words\n",
    "    binary=False, # do not binarize features\n",
    "    #max_features=1000,\n",
    "    min_df=0.001 # limit features to those that occur in at least 0.1% of articles\n",
    ")\n",
    "\n",
    "# perform vectorization\n",
    "X = count_vectorizer.fit_transform(news['body'])\n",
    "\n",
    "# vectorized shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what do the columns measure?\n",
    "count_vectorizer.get_feature_names_out()[100:110] # if old sklearn, use get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total word count in our corpus?\n",
    "X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus isn't very big. Or, well, the documents are pretty short, on average: 3.2M words / 127k documents = about 25 words per document. This fact will make classification a little harder than it would be if we had full documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify sports (vs everything else)\n",
    "\n",
    "We'll try **Bernouli naïve Bayes**, **Multinomial naïve Bayes**, and **Gaussian naïve Bayes** classifiers. The difference is that the Bernouli version first transforms our input features into ones and zeros, thereby coding for the presence or absence of each word in each document (but not accounting for how many times the word occurs in the document). Multinomial NB does not binarize inputs, so accounts for the number of occurrences. But Multinomial NB *does* expect nonnegative integers as input. Gaussian NB works with continuous inputs.\n",
    "\n",
    "There's no principled reason to prefer Bernouli, multinomial, or Gaussian NB. Use whichever one produces better results on your data (and is suitable for the type of input data you're using). Bernouli can be useful in the case where most documents use any word only a few times at most, rare documents use some words very many times, but you don't consider the rare, high-usage documents to be fundamentally unlike the other documents that use the same words just a few times.\n",
    "\n",
    "**Remember:** when we call `.fit()` (as we do explicitly here, or under the hood of `cross_val_score`, for example), what we're learning are the empirical probabilities of each word in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make boolean array of sports/other labels\n",
    "y = news['label'] == 'Sports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# --> limit to 1000 articles for demo purposes <--\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:1000], y[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# score BernouliNB classifier on train and test data\n",
    "clf = BernoulliNB().fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\"Performance on TRAINING data\")\n",
    "print(classification_report(y_train_pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"Performance on TEST data\")\n",
    "print(classification_report(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is evidence of overfitting in this case, because we perform significantly better when we make preditions on the training data than when we do the same on held-out test data\n",
    "\n",
    "How about in the multinomial case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# score Multinomial classifier on train and test data\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_train_pred = clf.predict(X_train)\n",
    "print(\"Performance on TRAINING data\")\n",
    "print(classification_report(y_train_pred, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "print(\"Performance on TEST data\")\n",
    "print(classification_report(y_test_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial NB performs just a tiny bit better in this case, but not enough to matter, really. And we're obviously still overfitting.\n",
    "\n",
    "Aside: One reason to like NB classifiers is that they're really fast. Sklearn's NB classifiers are over 100 times faster than logistic regression on the same task (and they produce comparable accuracy). This equivalent accuracy won't always be true, of course, but the speedup can be a big plus with large datasets.\n",
    "\n",
    "Bernouli NB is slower than multinomial NB due to binarization overhead. But the difference is trivial here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the most informative features\n",
    "\n",
    "Let's see how well we can do with lower-dimension inputs. We'll do this in two ways:\n",
    "\n",
    "1. Rank the input features (token counts) according to the degree to which their presence/absence matches the presence/absence of the `sports` class label. High mutual information between tokens and class labels suggests that a token will be a useful feature for our classifier. We'll also evaluate the *F*-statistic as a criterion and try out permutation importance, too.\n",
    "\n",
    "2. We'll also try dimension reduction via Truncated SVD, as we studied in previous lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# calculate mutual info scores btw each feature and the target label\n",
    "%time mi = mutual_info_classif(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get indices of mi array, sorted from highest to lowest mutual info\n",
    "mi_indices = np.flip(mi.argsort()) # np.flip reverses order; want high -> low\n",
    "mi_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discuss**: What do you expect to be some of the most informative features (words) for the **sports** catengory? That is, what words occurs often in sports stories, but not very often in other kinds of news articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the single most informative sports feature\n",
    "count_vectorizer.get_feature_names_out()[810]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sort the feature data and column labels\n",
    "X_train_sorted = X_train[:, mi_indices]\n",
    "X_test_sorted = X_test[:, mi_indices]\n",
    "features_sorted = np.array(count_vectorizer.get_feature_names_out())[mi_indices]\n",
    "features_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# performance with top 100 features\n",
    "clf = MultinomialNB().fit(X_train_sorted[:,:100], y_train)\n",
    "y_pred = clf.predict(X_test_sorted[:,:100])\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we've suffered just three points of weighted average F1 decrease despite throwing away all but 100 out of more than 3,500 original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how perforamnce responds in input feature dimensionality in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# how does accuracy respond to number of features used?\n",
    "num_features = [2**i for i in range(1,10)]\n",
    "f1s = []\n",
    "for i in num_features:\n",
    "    clf = MultinomialNB().fit(X_train_sorted[:,:i], y_train)\n",
    "    y_pred = clf.predict(X_test_sorted[:,:i])\n",
    "    f1s.append(f1_score(y_pred, y_test, average='weighted'))\n",
    "\n",
    "plt.scatter(num_features, f1s)\n",
    "plt.xlabel(\"Number of features used\")\n",
    "plt.ylabel(\"f1 score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In practice\n",
    "\n",
    "Split into train and test, fit a model, predict class labels, and examine classification errors. This is a typical workflow in production.\n",
    "\n",
    "We can make things a little easier by using the `SelectKBest` method, which handles all of the sorting and indexing that we performed by hand above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# using the n most-informative features\n",
    "\n",
    "# fit selector\n",
    "n_features = 100\n",
    "selector = SelectKBest(k=n_features, score_func=mutual_info_classif).fit(X_train, y_train)\n",
    "\n",
    "# select best features\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# fit and predict\n",
    "clf = MultinomialNB().fit(X_train_selected, y_train)\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "# examine performance\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select erroneous predictions\n",
    "errors = y_test != y_pred             # boolean array of gold/pred mismatches\n",
    "errors = np.where(errors==True)       # index array of test set error locations\n",
    "error_index = y_test.index[errors[0]] # map test-set error positions to original indices\n",
    "\n",
    "news_errors = news.loc[error_index, ['label', 'body']] # select errors from original data\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 0): # display errors\n",
    "    display(news_errors.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Truncated SVD\n",
    "\n",
    "Compare performance using SVD in place of most-informative features.\n",
    "\n",
    "In lecture, we talked about **standardizing** our input features before performing SVD, so that the variance isn't dominated simply by high-frequency words (in this case, words like \"and\" and \"the\" - we didn't perform stopword removal). Here, we're going to skip that step, so that we can preserve input sparsity (hence, computational and memory efficiency).\n",
    "\n",
    "If we *were* going to standradize our features, we'd do something like:\n",
    "\n",
    "```\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X.toarray()) # cast input to dense array\n",
    "```\n",
    "We would need to transform our input matrix to dense format with `.toarray()`, since `StandardScaler` refuses to break sparsity. And if we were going to do all that, we'd be performing PCA anyway, so we'd just use PCA in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reduce input dimensionality\n",
    "reducer = TruncatedSVD(n_components=100)\n",
    "X_train_reduced = reducer.fit_transform(X_train)\n",
    "X_test_reduced = reducer.transform(X_test)\n",
    "X_train_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# examine new features for the first document\n",
    "X_train_reduced[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# performace using top SVD components\n",
    "clf = GaussianNB().fit(X_train_reduced, y_train)\n",
    "y_pred = clf.predict(X_test_reduced)\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use `GaussianNB`, because multinomial NB (as the name suggests) expects purely positive, integer-valued inputs. Gaussian NB accepts all real-valued inputs.\n",
    "\n",
    "Performance here isn't as good as it was when we used the top 100 **sports-specific** features (recall that our task is **sports** classification). But the SVD features were more than an order of magnitude faster to calculate and, more importantly, should be useful if we wanted to classify any of the other categories (because the SVD dimensions capture overall variance in the data, rather than being tied to any specific category). \n",
    "\n",
    "As it turns out *in this case*, that isn't really true: top sports features outperform SVD in the sports case, but are surprisingly close to SVD performance for the other categories, too. Think about why this might be true in the special case of language data? (High dimensionality, correlated features, lots of nonuniform structure in the data; 100 features are a *lot* for just 1k observations). If you're interested, you might try to implement these evaluations for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_selected, y_train)\n",
    "\n",
    "r = permutation_importance(\n",
    "    clf, \n",
    "    X_test_selected.toarray(), # expects dense input \n",
    "    y_test,\n",
    "    scoring='f1_weighted',\n",
    "    n_repeats=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Permutation feature importance:\\n\")\n",
    "\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "    # only display significant features\n",
    "    if r.importances_mean[i] - 1 * r.importances_std[i] > 0:\n",
    "        word = count_vectorizer.get_feature_names_out()[int(selector.get_feature_names_out()[i].strip('x'))]\n",
    "        print(f\"{word:>10}  {r.importances_mean[i]:.3f} +/- {r.importances_std[i]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
