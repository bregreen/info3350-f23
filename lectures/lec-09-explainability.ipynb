{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 09: Model inspection and explainability\n",
    "\n",
    "## To do\n",
    "\n",
    "* Problem set 2 was due last night; now burning slip days if not yet submitted\n",
    "* No section on Friday\n",
    "    * Happy Fall break\n",
    "* Reading for this week\n",
    "    * Read articles by Underwood and by Barron et al.\n",
    "    * Response post due Tuesday, 4:00pm (NetIDs Q-Z)\n",
    "* Problem set 3 will be released on Friday\n",
    "    * Due in two weeks, i.e., Thursday, 10/19 at 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explaining your model\n",
    "\n",
    "### What does it mean to \"explain\" a model?\n",
    "\n",
    "**Can you tell someone else (or yourself!) how it works?** It's more than just the algorithm, though understanding the algorithm is part of it ...\n",
    "\n",
    "* What are your inputs?\n",
    "* Where do your data come from?\n",
    "* What algorithm did you use?\n",
    "* How did you select your parameters (for preprocessing, algorithm selection, hyperparameters, etc.)\n",
    "* How accurate are your results?\n",
    "* On what kinds of objects does the model perform well or badly?\n",
    "* What features most impact classification accuracy?\n",
    "    * Overall?\n",
    "    * For any given object?\n",
    "\n",
    "### Why is explainability important?\n",
    "\n",
    "In general:\n",
    "\n",
    "* Can you convince someone (or yourself) that your model is trustworthy?\n",
    "* Are there errors or anomalies in your data/processing/code?\n",
    "    * Consider a British/American text classifier that uses frequency of \"colour\" or \"lorry\"\n",
    "    * Or a news article topic classifier that includes source names like \"The Wall Street Journal\" or \"The Sporting News\" in its input\n",
    "        * Not wrong, but fragile, not generalizable\n",
    "* When the classifier makes mistakes, why did it fail?\n",
    "* Is your classifer biased?\n",
    "    * Are the features driving your model ones that you trust? Or that you *should* use?\n",
    "    * Consider ZIP code or prior arrest for predicting recidivism\n",
    "    \n",
    "In some cases:\n",
    "\n",
    "* Maybe you care about the features\n",
    "    * Consider medical treatment. Give five drugs, which ones drive recovery?\n",
    "* Maybe the features help you understand the (high-level) phenomenon you're modeling\n",
    "    * For example, authorship attribution via stopword frquency\n",
    "\n",
    "### Must you care about explaining your model?\n",
    "\n",
    "* Probably, but sometimes more than others\n",
    "* Things that decrease the need for explainability:\n",
    "    * You only care about accuracy/performance\n",
    "    * You will only ever work with one dataset\n",
    "        * And you have high confidence in its quality\n",
    "    * The stakes of your classification are low\n",
    "        * No one is harmed if/when you're wrong\n",
    "    * The costs of modeling are low (so that you don't need to justify your existence to a funder)\n",
    "* This is all to say: **If your problem is important, explainability is important**\n",
    "    \n",
    "## Types of explanation\n",
    "\n",
    "### Intrinsic and model-specific\n",
    "\n",
    "* Does your model produce individual feature weights or decision criteria?\n",
    "    * Linear models, trees\n",
    "* Do you know in advance which model(s) you'll use?\n",
    "    * Recall we noted that decision trees are popular because they are so easily interpretable, even when they rarely offer best performance\n",
    "\n",
    "### *Post hoc* and model-agnostic\n",
    "\n",
    "* Is your model a \"black box\"?\n",
    "    * Ensembles, neural networks\n",
    "* Do you want to be able to use/interpret/explain an arbitrary classifier?\n",
    "\n",
    "### Local or global?\n",
    "\n",
    "* Do you need to identify the most important features overall? (global)\n",
    "* Do you need to be able to identify the most important features for an individual classification? (local)\n",
    "\n",
    "## Some approaches\n",
    "\n",
    "### Linear methods\n",
    "\n",
    "* Examples: Logistic regression, Lasso, Ridge\n",
    "* Intrinsic, global\n",
    "* Many `sklearn` classifiers have a `.coef_` attribute of the trained classifer object. \n",
    "* This provides the coefficients of each feature in the input matrix.\n",
    "\n",
    "### Trees\n",
    "\n",
    "* Example: Decison tree, Random forest (by extension)\n",
    "* Intrinsic, global\n",
    "* Has an attribute `feature_importances_`\n",
    "* Reflects the degree to which each feature separates the classes\n",
    "    * \"How much does selecting on a feature reduce the impurity of the classes?\"\n",
    "\n",
    "### Permutation\n",
    "\n",
    "* Can be used with black box models\n",
    "* Is *post hoc*\n",
    "* Note that this is one version of what we mentioned last week when we mentioned *post hoc* feature selection\n",
    "* In brief:\n",
    "    * Measure model performance (accuracy, f1, etc.)\n",
    "    * Shuffle (permute) the values of one feature for all the objects \n",
    "        * This renders the feature non-informative\n",
    "    * Measure model performance again\n",
    "        * It will drop, unless the feature was non-informative to begin with (in which case, it'll stay about the same)\n",
    "    * Repeat for all features\n",
    "    * The features that, when shuffled, produce the largest drop in classification performance are the most important ones\n",
    "* Note that permutation in this case shouldn't be confused with a permutation test as used to estimate the *p* value for a hypothesis test (on which, more next week).\n",
    "\n",
    "### Partial dependence\n",
    "\n",
    "* Often most useful for regression, but can be adapted to classification\n",
    "* Asks: by how much does the response variable change when I make a small change to the input variable across a range of possible input values?\n",
    "    * For classification, need to measure something like the change in class probability\n",
    "    * Note that some `sklearn` classifiers (including `RandomForestClassifier` and `LogisticRegression`) include a `.predict_proba()` method to predict probabilities\n",
    "        * Output of `predict_proba` is a vector of class probabilities rather than a single (most likely) class label for each object\n",
    "* Can help to identify non-linear relationships between input and reponse\n",
    "    * For example, bike rentals are a function of temperature (among other things)\n",
    "    * Rentals go up with increasing temperature ...\n",
    "    * ... until it becomes too hot (at which point they decrease with temperature)\n",
    "* Not often super useful for token-based work, but can be more relevant for embeddings and other dimension-reduced representations\n",
    "\n",
    "## Packages \n",
    "\n",
    "* Scikit-learn\n",
    "    * [sklearn.inspection.permutation_importance](https://scikit-learn.org/stable/modules/permutation_importance.html)\n",
    "    * [sklearn.inspection.partial_dependence](https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py)\n",
    "* [lime](https://github.com/marcotcr/lime)\n",
    "* [ELI5](https://eli5.readthedocs.io/en/latest/overview.html)\n",
    "* [SHAP](https://shap.readthedocs.io/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News demo\n",
    "\n",
    "About 125,000 news articles, distributed evenly across four categories (world, business, sports, and science/technology), each trimmed to contain **just the first sentence or two of the original article**. [Data source](https://github.com/mhjabreel/CharCnn_Keras/tree/master/data/ag_news_csv), plus minor massaging into current format.\n",
    "\n",
    "This is the same dataset we used to discuss feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and clean news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from   sklearn.feature_extraction.text import CountVectorizer\n",
    "from   sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "from   sklearn.linear_model import LogisticRegressionCV # NB. CV version; autotune hyperparameters\n",
    "from   sklearn.metrics import classification_report\n",
    "from   sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data from disk and examine\n",
    "\n",
    "news = pd.read_csv(os.path.join('..', 'data', 'news', 'news_text.csv.gz'))\n",
    "\n",
    "# a function to get rid of datelines at the start of articles\n",
    "#  matches one or more hyphens or colons in first 40 chars,\n",
    "#  drops everything before that match (plus the match itself)\n",
    "pattern = '[-:]+ '\n",
    "matcher = re.compile(pattern) # compiled regexs are faster\n",
    "\n",
    "def remove_dateline(text, matcher=matcher):\n",
    "    '''\n",
    "    Remove source names and datelines from a text string\n",
    "    If there is a hyphen or colon in the first 40 characters, \n",
    "      drops everything before the hyphen(s)/colon(s)\n",
    "    If no hyphen/colon, do nothing\n",
    "    Return processed string\n",
    "    '''\n",
    "    result = matcher.search(text, endpos=40)\n",
    "    if result:\n",
    "        return text[result.end():]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# clean article text\n",
    "news['body'] = news['body'].apply(remove_dateline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up a vectorizer object\n",
    "count_vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode', # collapse accents to base character\n",
    "    stop_words=None, # do not remove stop words\n",
    "    binary=False, # do not binarize features\n",
    "    #max_features=1000,\n",
    "    min_df=0.001 # limit features to those that occur in at least 0.1% of articles\n",
    ")\n",
    "\n",
    "# perform vectorization\n",
    "X = count_vectorizer.fit_transform(news['body'])\n",
    "\n",
    "# vectorized shape\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify sports (vs everything else)\n",
    "\n",
    "We'll use a few different classifiers to demonstrate different approaches to model inspection and explainability ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make boolean array of sports/other labels\n",
    "y = news['label'] == 'Sports'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and test on selected examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "# --> limit articles for demo purposes <--\n",
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(X[:2000], y[:2000], np.arange(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intrinsic examination of logistic regression on selected features\n",
    "\n",
    "We're really pruning back our features so that the model is easier to inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logit using the n most-informative features\n",
    "\n",
    "# fit selector\n",
    "n_features = 20\n",
    "selector = SelectKBest(k=n_features, score_func=mutual_info_classif).fit(X_train, y_train)\n",
    "\n",
    "# select best features\n",
    "X_train_selected = selector.transform(X_train)\n",
    "X_test_selected = selector.transform(X_test)\n",
    "\n",
    "# fit and predict\n",
    "clf = LogisticRegressionCV(scoring='f1_weighted', max_iter=500).fit(X_train_selected, y_train)\n",
    "y_pred = clf.predict(X_test_selected)\n",
    "\n",
    "# examine performance\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# what features have we used?\n",
    "features_used = count_vectorizer.get_feature_names_out()[[int(i.strip('x')) for i in selector.get_feature_names_out()]]\n",
    "print(features_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(clf.coef_.shape[1]):\n",
    "    print(f\"{features_used[i]:<10} {clf.coef_[0,i]:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "plt.barh(list(range(X_train_selected.shape[1])),clf.coef_[0], tick_label=features_used)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(max_features=100, max_depth=30).fit(X_train, y_train)\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "\n",
    "# examine performance\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "importance_idx = np.flip(dt_clf.feature_importances_.argsort())\n",
    "\n",
    "for i in range(20):\n",
    "    print(f\"{count_vectorizer.get_feature_names_out()[importance_idx[i]]:<12} {dt_clf.feature_importances_[importance_idx[i]]:6.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plot_tree(\n",
    "    dt_clf, \n",
    "    max_depth=3, \n",
    "    feature_names=[i for i in count_vectorizer.get_feature_names_out()],\n",
    "    class_names=['Other', 'Sports'],\n",
    "    filled=True,\n",
    "    rounded=True\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model interpretation with `shap`\n",
    "\n",
    "A game-theoretical approach to feature model examination. See the [shap documentation](https://github.com/slundberg/shap).\n",
    "\n",
    "`conda install -c conda-forge shap`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# build a dataframe for ease of use\n",
    "df = pd.DataFrame(\n",
    "    data=X_test_selected.toarray(), \n",
    "    columns=features_used,\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set up explainer and calculate shap values across multiple resamplings\n",
    "explainer = shap.Explainer(clf, df)\n",
    "shap_values = explainer(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the waterfall_plot shows how we get from shap_values.base_values to model.predict(X)[sample_ind]\n",
    "sample_to_explain = 20\n",
    "print(f\"True class: {news.label.iloc[idx_test[sample_to_explain]]}.\\n{news.body.iloc[idx_test[sample_to_explain]]}\")\n",
    "shap.plots.waterfall(shap_values[sample_to_explain], max_display=len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grey numbers in figure are feature values\n",
    "df.iloc[sample_to_explain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mean importance for each feature\n",
    "shap.plots.bar(shap_values, max_display=len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# better yet, distributions over importances for all cases\n",
    "shap.plots.beeswarm(shap_values, max_display=len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
