{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 3350/6350\n",
    "\n",
    "## Lecture 11: Topic models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From words to topics\n",
    "\n",
    "* If we want to know what our documents are about -- that is, if we want to measure their content -- we can count the words in them\n",
    "* Words are good, but imperfect, features, as noted in previous lectures\n",
    "    * Words split one subject or topic across multiple features\n",
    "        * Example: The subject \"pets\" can be materialized as \"cat\", \"kitten\", \"dog\", \"parakeet\", etc.\n",
    "    * Words also *fail* to split subjects that are distinct\n",
    "        * Example: \"fall\" is a season, a physical action, and/or a state of defeat or disgrace\n",
    "        * This is called *polysemy* (literally, \"many signs\")\n",
    "    * Lemmatization helps a little with these issues, but not as much as we'd like\n",
    "* We would (sometimes) like a way to replace words with subjects or topics\n",
    "    * If there are fewer distinct topics than there are distinct words, we would thus trade specificity for generality\n",
    "* Words -> topics would help us in two ways ...\n",
    "    * Identifying topics would give us a better sense of what's in our documents\n",
    "        * Related to search and information retrieval\n",
    "            * How do you search if you don't know what's in your documents?\n",
    "            * Or what if the documents use nonstandard language or talk around an issue?\n",
    "        * It's easy to forget how often we have this problem in text analysis\n",
    "        * Corpora are usually *big*; if you could read them, you wouldn't be using text mining to study them\n",
    "        * *Exploratory* analysis is often warranted\n",
    "    * Transforming words into topics would help with overfitting in our models by reducing the dimensionality of our feature space\n",
    "        * This is a big deal, too\n",
    "        * We can reduce dimensions, but the reduced version often becomes hard to interpret\n",
    "        * Reduced dimensions that correspond to distinct subject matter help with explainability\n",
    "        \n",
    "## Topic models\n",
    "\n",
    "* A topic model is any model of a corpus that has a mathematical representation of:\n",
    "    * **Documents:** Containers of text that are composed from a limited number of topics\n",
    "    * **Topics:** Sets of words or other tokens (or probability distributions across a vocabulary) that together are used to express a (coherent?) subject or idea\n",
    "    * **Words:** Specific tokens from which topics and documents are built\n",
    "* Topic models are **unsupervised**\n",
    "    * This is not a classification task. We don't know in advance what are the \"correct\" topics, nor what words belong to which topic, nor how much of each topic is in each document.\n",
    "* Topic models generally operate on word counts (so-called \"bags of words\"), hence do not require full, sequential texts\n",
    "    * This is useful if, for some reason, you can get word counts but not full texts (copyright laws, typically)\n",
    "* The earliest model that might be described as a topic model was called **Latent Semantic Analysis (LSA)**\n",
    "    * In short: Truncated SVD to reduce dimensionality\n",
    "        * Each dimension is a topic\n",
    "    * Then examine the feature weights within each dimension\n",
    "        * Feature weights tell you how much of each word is captured by each dimension\n",
    "    * Each document contains some amount of each dimension\n",
    "        * These are your document topic proportions\n",
    "    * Can perform document similarity measurement on topic composition via the usual distance metrics (often, cosine distance)\n",
    "* LSA tends to produce \"diffuse\" topics, since it has no mechanism to encourage sparsity\n",
    "    * That is, topics contain non-trivial weights for a lot of different words\n",
    "    * And documents contain nontrivial amounts of many topics\n",
    "    * Good similarity performance, poor interpretability\n",
    "* Now, we generally use **Latent Dirichlet Allocation (LDA)** to produce **probabilistic topic models**\n",
    "    * The Dirichlet distribution allows us to parameterize \"*concentration*\", that is, the degree to which the topical distribution in each document is expected to be sparse\n",
    "    * Topical sparsity has big advantages when it comes to interpretability\n",
    "    * See today's reading for the mathematical details\n",
    "        * The math isn't super hard, but it's also not our immediate concern\n",
    "\n",
    "## The standard model of LDA\n",
    "\n",
    "Or, where do baby documents come from?\n",
    "\n",
    "We need a data-generating process that we can model. So, imagine we're a decent programmer and terrible fiction writer. How would we write a book?\n",
    "\n",
    "[Hand sketch ...]\n",
    "\n",
    "* Note that **no one thinks writers work this way** (except maybe for language poets)\n",
    "* A model doesn't have to be correct to be useful\n",
    "\n",
    "## LDA in `sklearn`\n",
    "\n",
    "* There are lots of topic modeling packages, each of which implements some shared features and many of which introduce advanced variations (topics over time, authorless models, semi-supervised topics, etc.)\n",
    "* MALLET is one of the oldest and best topic modeling packages\n",
    "    * Maintained by David Mimno in Cornell IS!\n",
    "    * But it's in Java\n",
    "        * Java is fine, but a pain to use alongide Python\n",
    "    * Check out Prof. Mimno's [online topic modeling tool](https://mimno.infosci.cornell.edu/jsLDA/)\n",
    "* [tomotopy](https://bab2min.github.io/tomotopy/v/en/) is a pure-python implementation of much of MALLET\n",
    "* We'll use scikit-learn, since it integrates so seamlessly with our other workflows\n",
    "    * Adequately(?) fast, decent performance, lacks bells and whistles\n",
    "\n",
    "## Preprocessing matters\n",
    "\n",
    "* LDA topic models can be sensitive to preprocessing decisions\n",
    "* Chunking and stopword removal are important\n",
    "* Remember, the algorithm doesn't have a human \"concept\" of a topic\n",
    "    * Is a topic a specific idea? A way of speaking? A high-level theme? A genre of literature?\n",
    "        * The algorithm doesn't know!\n",
    "* The Dirichlet distribution \"wants\" to produce sparse topics\n",
    "    * If you use long documents that are \"about\" a lot of different things, your topics will be diffuse and \"smear-y\"\n",
    "    * If your documents are short and focused on one or two ideas, your topics will be cleaner and more interpretable\n",
    "        * Congressional policy speeches or press releases are good examples of whole documents that focus on one or two topics\n",
    "    * In many cases, chunking by paragraph, page, or several-hundred-word passage is appropriate\n",
    "    * But aggressive chunking can produce very large matrices, even in modest corpora\n",
    "        * May need streaming approaches with online updates to manage memory consumption\n",
    "* LDA doesn't need word order, so is well suited to \"non-consumptive\" or \"non-expressive\" use\n",
    "    * Sources: [HTRC extracted features](https://analytics.hathitrust.org/datasets), [JSTOR DfR](http://www.jstor.org/dfr)\n",
    "    * Hence usable with in-copyright texts that are only available as bags of words\n",
    "    * Preprocessing can be trickier with these, because you do not control tokenization or chunking\n",
    "    \n",
    "## The US State of the Union (SOTU) address, 1791-2018\n",
    "\n",
    "* An annual address by the president of the US to congress\n",
    "* Describes political issues, priorities, and programs\n",
    "* Corpus contains 227 speeches, total of about 1.75M words\n",
    "* A now-classic demo for topic models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from   glob import glob\n",
    "import os\n",
    "\n",
    "# Where's the data?\n",
    "corpus_dir = os.path.join('..', 'data', 'sotu')\n",
    "files = glob(os.path.join(corpus_dir, '*.txt'))\n",
    "\n",
    "# Read files and chunk by paragraphs\n",
    "texts = []\n",
    "years = []\n",
    "for file in files:\n",
    "    metadata = os.path.basename(file).split('.')[0]\n",
    "    year = int(metadata.split('_')[1]) # file names are President_Year.txt\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "        for paragraph in text.split('\\n\\n'): # Paragraph break is two newlines\n",
    "            texts.append(paragraph)\n",
    "            years.append(year)\n",
    "print(\"Number of SOTU texts:\", len(files))\n",
    "print(\"Number of paragraphs:\", len(texts))\n",
    "print(\"Years:\", min(years), 'through', max(years))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize\n",
    "from   sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer( # Token counts, not normalized (sklearn normalizes later)\n",
    "    input = 'content',\n",
    "    encoding = 'utf-8',\n",
    "    strip_accents = 'unicode',\n",
    "    stop_words='english', # uncomment to remove fixed stops from input\n",
    "    lowercase = True,\n",
    "    min_df = 0.001, # Remember that we've chunked by paragraph\n",
    "    max_df = 0.25    # Ditto\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(texts)\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Total vectorized words in the corpus:\", X.sum())\n",
    "print(\"Average vectorized paragraph length:\", int(X.sum()/X.shape[0]), \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from   sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=20, # Number of topics to find\n",
    "    n_jobs=-1,       # Use all CPU cores\n",
    "    verbose=1,       # Print progress\n",
    "    max_iter=10,     # Might want more in production work\n",
    "    evaluate_every=0 # Set >=1 to test for convergence (slow, but can stop iteration)\n",
    ")\n",
    "\n",
    "lda.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exmaine the fitted model directly\n",
    "print(lda.components_.shape)\n",
    "lda.components_[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for each of 20 topics, we have a distribution over the vocabulary. Ehh ... maybe we can do better for interpretability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top words per topic\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words, hide_stops=False):\n",
    "    if hide_stops:\n",
    "        from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = f\"Topic {topic_idx: >2}: \"\n",
    "        top_words_idx = topic.argsort()\n",
    "        if not hide_stops:\n",
    "            top_words = [feature_names[i]\n",
    "                         for i in top_words_idx[:-n_top_words - 1:-1]]\n",
    "        else:\n",
    "            top_words = []\n",
    "            i = 1\n",
    "            while len(top_words) < n_top_words:\n",
    "                if feature_names[top_words_idx[-i]] not in ENGLISH_STOP_WORDS:\n",
    "                    top_words.append(feature_names[top_words_idx[-i]])\n",
    "                i += 1\n",
    "        message += \" \".join(top_words)    \n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(lda, vectorizer.get_feature_names_out(), n_top_words=10, hide_stops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pyLDAvis\n",
    "#  Uncomment to run\n",
    "#import sys\n",
    "#!conda install --yes --prefix {sys.prefix} -c conda-forge pyldavis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.lda_model\n",
    "import warnings\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    lda_vis = pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='pcoa', sort_topics=False)\n",
    "pyLDAvis.display(lda_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** that `pyLDAvis` sorts topics in order of descending overall frequency in the corpus unless you pass `sort_topics=False`. This behavior can make it hard to use for exploratory work, so keep `sort_topics=False` in place unless you know what you're doing. And note that `pyLDAvis` has renumbered the topic identifiers to start from `1`, so be careful with alignment issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a doc-topic matrix\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    doc_topic_matrix = lda.transform(X)\n",
    "print(\"Doc-topic matrix shape:\", doc_topic_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine topic distribution in a random paragraph\n",
    "import random\n",
    "doc_idx = random.randrange(len(doc_topic_matrix))\n",
    "print(f\"Topic distributions in paragraph {doc_idx} ({years[doc_idx]})\")\n",
    "print(\"The text:\\n\", texts[doc_idx], \"\\n\")\n",
    "display(doc_topic_matrix[doc_idx])\n",
    "print()\n",
    "print_top_words(lda, vectorizer.get_feature_names_out(), n_top_words=10, hide_stops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation plot\n",
    "#  Adapted from Seaborn docs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from   sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#sns.set_context('talk')\n",
    "corr = np.corrcoef(doc_topic_matrix.T)\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(20, 230, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=mask, \n",
    "    cmap=cmap, \n",
    "    center=0,\n",
    "    square=True, \n",
    "    linewidths=.5, \n",
    "    cbar_kws={\"shrink\": .5}\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series\n",
    "n_cols = 4\n",
    "n_topics = doc_topic_matrix.shape[1]\n",
    "n_rows = n_topics/n_cols\n",
    "if n_rows%1!=0:\n",
    "    n_rows += 1\n",
    "n_rows = int(n_rows)\n",
    "\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(24,12))\n",
    "axs = axs.ravel()\n",
    "\n",
    "valid_years = sorted(np.unique(years))\n",
    "for topic in range(n_topics):\n",
    "    topic_data = doc_topic_matrix[:,topic]\n",
    "    topic_means = []\n",
    "    for year in valid_years:\n",
    "        mask = np.equal(years, year)\n",
    "        topic_mean = np.mean(topic_data[mask])\n",
    "        topic_means.append(topic_mean)\n",
    "    axs[topic].plot(valid_years, topic_means)\n",
    "    axs[topic].set_title(f'Topic {topic}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the strong temporal trends in topic frequency. Maybe we can predict speech date from topical content?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict speech date from topic content\n",
    "from   sklearn.linear_model import LinearRegression\n",
    "from   sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Fit and predict using topics\n",
    "X_topics = StandardScaler().fit_transform(doc_topic_matrix)\n",
    "predictor = LinearRegression().fit(X_topics, years)\n",
    "y_pred = predictor.predict(X_topics)\n",
    "\n",
    "# Score\n",
    "print(\"Mean cross-validated R^2 (topics):\", round(np.mean(cross_val_score(LinearRegression(), X_topics, years, scoring='r2', cv=10)),3))\n",
    "\n",
    "# Plot\n",
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "sns.regplot(x=years, y=y_pred, scatter_kws={'alpha':0.1})\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Topics as features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ditto, using 20 best word features\n",
    "from   sklearn.feature_selection import SelectKBest, mutual_info_regression, f_regression\n",
    "\n",
    "# Select best word features\n",
    "method = f_regression\n",
    "selector = SelectKBest(method, k=20)\n",
    "X_words = selector.fit_transform(X, years)\n",
    "\n",
    "# Score\n",
    "print(\"Mean cross-validated R^2 (words):\", round(np.mean(cross_val_score(LinearRegression(), X_words, years, scoring='r2', cv=10)),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting by topics performs **much** better than predicting by the same number of \"best\" word features!\n",
    "\n",
    "Also: `mutual_info_regression` is much more robust than is `f_regression` as a measure of association (F measures linear relationships between inputs and outputs; MI in nonparametric and can capture any relationship), but the former is also much slower to calculate (like, c. 1000x) than is the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ditto, using SVD with 20 components\n",
    "from   sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Find top 20 SVD components\n",
    "reducer = TruncatedSVD(n_components=20)\n",
    "X_svd = reducer.fit_transform(X)\n",
    "\n",
    "# Score\n",
    "print(\"Mean cross-validated R^2 (SVD):\", round(np.mean(cross_val_score(LinearRegression(), X_svd, years, scoring='r2', cv=10)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
