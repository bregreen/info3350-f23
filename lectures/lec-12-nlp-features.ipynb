{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INFO 3350/6350\n",
    "\n",
    "## Lecture 12: Feature expansion and natural language processing (NLP)\n",
    "\n",
    "## To do\n",
    "\n",
    "* Make sure you're read the spaCy docs assigned for last week\n",
    "    * Optional: Jurafsky and Martin, ch. 8 (POS, NER)\n",
    "* Next up: static word embeddings\n",
    "* Tuesday/Friday: Nelson (on embeddings to study historical biases)\n",
    "    * Respond by tomorrow (Tuesday) afternoon if it's your week (H-P)\n",
    "* PS3 due Thursday night by 11:59pm\n",
    "    * \"Legibly\" means \"easy to read\"\n",
    "    * If we ask for discussion, we expect thoughtful analysis\n",
    "    * Reports of problems with VSCode losing cell output\n",
    "* Undergrad sections are CANCELED this week. Grad section is ON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we want from our features\n",
    "\n",
    "* Sometimes we want to move from specific words to higher-level categories in order to achieve **better classification or similarity results**\n",
    "    * Here, we can be pragmatic. What features produce the best results?\n",
    "    * This is an engineering problem\n",
    "    * But we may still care about explainability\n",
    "* Sometimes we move from words to concepts because we care about the **concepts themselves**\n",
    "    * In this case, explainability is really important\n",
    "    * We need to understand how our features capture (or fail to capture) the concepts we care about\n",
    "* And sometimes, we want to incorporate **paratextual features**, that is, features that are *about* a book, but aren't *in* the book, strictly speaking\n",
    "    * Author identity, gender, publication date, national origin, genre, market success, etc.\n",
    "\n",
    "**Today: look (a little bit) into non-unigram feature types that may help us in each of these cases**\n",
    "\n",
    "## There's more to life than token unigrams\n",
    "\n",
    "* Most of our work so far has used counts of **token unigrams** (that is, individual words) to characterize texts\n",
    "* This is non-crazy!\n",
    "    * The words in a book tell us a lot about that book\n",
    "* But we often care less about words than we do about the classes of things those words represent\n",
    "    * Recall our work on sentiment and emotion\n",
    "        * We collapsed thousands of words into a small set of distinct(?) emotion types\n",
    "    * Words are often just the most straightforward way to capture ideas, subject matter, types of action, chacaterization, etc.\n",
    "* There are additional ways to capture these higher-level concerns\n",
    "    * ***n*-grams** and **noun phrases** capture multi-word sequences like \"best friend\" and \"New York\"\n",
    "    * **Lemmatization** collapses specific word forms into a single root (\"running\" -> \"run\", \"cats\" -> \"cat\")\n",
    "    * **Part of speech** tagging collapses words into their linguistic functions (noun, verb, adjective, preposition, etc.)\n",
    "    * **Named entity recognition** identifies named entities (people, organizations, places, etc.)\n",
    "    * Several different approaches to identifying **subject matter**\n",
    "        * Topic models\n",
    "        * Latent Semantic Analysis (LSA), which we've already seen, in a different context, as SVD\n",
    "        * Coming up: word embeddings\n",
    "* Can think of **all** these methods as potential *dimension reduction* techniques: we use far more words than we have truly distinct concepts\n",
    "    \n",
    "## *n*-grams\n",
    "\n",
    "* ***n*-grams** are sequences of some number of words that occur one after another in a text\n",
    "* '*n*' represents the number of consecutive words.\n",
    "    * *n*=1 is called a unigram. We've used these extensively already.\n",
    "    * *n*=2 is a bigram, 3=trigram, etc.\n",
    "\n",
    "Note a useful tool: [Google Books Ngram Viewer](https://books.google.com/ngrams). Also, [see a sample](https://books.google.com/ngrams/interactive_chart?content=Great+War%2CWorld+War+I%2CWorld+War+II&year_start=1800&year_end=2019&corpus=27&smoothing=3&direct_url=t1%3B%2CGreat+War%3B%2Cc0%3B.t1%3B%2CWorld+War+I%3B%2Cc0%3B.t1%3B%2CWorld+War+II%3B%2Cc0) that demonstrates an interesting historical issue..\n",
    "\n",
    "Of course, **be careful when considering historical use of words and phrases**. Consider semantic drift (changes in a word's meaning over time), orthographic issues like the (archaic) long 's', and the general sense in which published books do no represent all of a society in real time. But these are issues we always face! \n",
    "\n",
    "We can do this, too. As ever, we can count sequences of words by hand. But `sklearn`'s vectorizers make it easy to add *n*-gram features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from   sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\n",
    "    \"I grew up in New York\",\n",
    "    \"Ada Lovelace was from the United Kingdom\",\n",
    "    \"Cats and dogs can be friends\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    stop_words='english',\n",
    "    ngram_range=(2,2) # <- retain bigrams (alone) after stopword removal\n",
    ")\n",
    "\n",
    "features = vectorizer.fit_transform(sentences).todense()\n",
    "\n",
    "# Create a dataframe for easy display\n",
    "df = pd.DataFrame(features, columns=vectorizer.get_feature_names_out())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that we've removed stopwords and retained bigrams (2-grams) only\n",
    "* We could also retain any range of *n*-grams, including unigrams\n",
    "    * In fact, `CountVectorizer` by default uses `ngram_range=(1,1)`\n",
    "* Bigrams can feel like they capture people, places, and other named entities or noun phrases, but there are better ways to accomplish those specific tasks\n",
    "* Still, *n*-grams can be useful features\n",
    "    * Unlike the NLP-derived features discussed below -- which generally reduce the dimensionality of our data -- using *n*-gram features tends to increase dimensionality\n",
    "    * Beware rapidly expanding feature matrices, especially with *n*>2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic NLP tasks with `spaCy`\n",
    "\n",
    "* **Lemmatization, part of speech tagging, named entity recognition, noun phrase detection, dependency parsing**, etc.\n",
    "* Pretty much all of these involve using text sequence information and hand-labeled training data to learn how to predict the class to which a token belongs\n",
    "    * By \"text sequence information,\" we mean that for each token in order as we move through a text, we look at one or more tokens before or after that token in order to infer things about it\n",
    "    * In general, we are trying to produce the label with the maximum likelihood, given what we know about the labels of the tokens around our target token\n",
    "* NLP is a big subject\n",
    "    * In the near term, see [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/), especially [ch. 8](https://web.stanford.edu/~jurafsky/slp3/8.pdf) as (optionally) assigned for today\n",
    "    * In the longer run, consider taking a class with Claire Cardie (CS/IS) or another of the [Cornell NLP faculty members](https://nlp.cornell.edu/people/)\n",
    "* Several packages to accomplish many NLP tasks. Two widely used ones for Python:\n",
    "    * NLTK (Natural Language ToolKit) is a classic in Python\n",
    "        * We used its sentence splitting and word tokenization features earlier in the course\n",
    "        * Pros: easy, pythonic\n",
    "        * Cons: slow, not state-of-the-art performance\n",
    "    * SpaCy\n",
    "        * Newer, neural-network based\n",
    "        * Good speed and performance\n",
    "        * Pretty easy to use\n",
    "      \n",
    "### Install `spaCy` and associated data\n",
    "\n",
    "Only need to do this once for your installation, not every (subsequent) time you use the library. You should have it already if you installed the [course package set](https://github.com/wilkens-teaching/info3350-f23/tree/main/setup). If not, you'll need to run:\n",
    "\n",
    "```\n",
    "conda install -c conda-forge spacy -y\n",
    "python -m spacy download en_core_web_sm\n",
    "python -m spacy download en_core_web_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some NLP basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Sample sentence\n",
    "cornell = 'Cornell is a private, Ivy League university and the land-grant university for New York state.'\n",
    "\n",
    "# Process the document\n",
    "doc = nlp(cornell)\n",
    "\n",
    "# Examine the processed document\n",
    "print(doc)\n",
    "print(type(doc))\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens, POS tags, dependency\n",
    "\n",
    "Note that SpaCy's model gives us tokens (among other things), each of which has properties attached to it. So, if we want to know what a token is (that is, what its text is), we refer to `token.text` (assuming our token is stored in a variable named `token`). If we want its part of speech, that's `token.pos_`. Its lemma: `token.lemma_`.\n",
    "\n",
    "Note that these POS tags are a reduced set (called UPOS). If you want the full Penn Treebank tagset, see the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print tokens, POS tags, and dependency info\n",
    "fmt = \"{:>12}\"*3 # three, right-justified, 12-character-wide columns\n",
    "for token in doc:\n",
    "    print(fmt.format(token.text, token.pos_, token.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the root, subject, and direct object of the example sentence\n",
    "for sent in doc.sents:\n",
    "    root = sent.root\n",
    "    subject = None\n",
    "    dir_obj = None\n",
    "    for child in root.children:\n",
    "        if child.dep_ == 'nsubj':\n",
    "            subject = child\n",
    "        if child.dep_ == 'dobj':\n",
    "            dir_obj = child\n",
    "    print(\"Root:\", root)\n",
    "    if subject:\n",
    "        print(\"Subject:\", subject)\n",
    "    if dir_obj:\n",
    "        print(\"Direct object:\", dir_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# explain a tag\n",
    "spacy.explain('dobj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about why you might want to navigate a dependency tree like this ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmas and POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gets lemmas and POS tags\n",
    "for token in doc:\n",
    "    print(fmt.format(token.text, token.tag_, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that lemmatization is a useful preprocessing step during vectorization. Think about how you would write a preprocessing function using `spaCy` that would integrate with an `sklearn` vectorizer via the `preprocessor` option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entities\n",
    "\n",
    "Entities aren't strictly token-level properties, so we don't retrieve them by calls to the proerties of individual tokens. Instead, we iterate over `doc.ents`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Entities\n",
    "for ent in doc.ents:\n",
    "    print(fmt.format(ent.text, ent.label_, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the full set of [named entity types](https://spacy.io/models/en) in SpaCy's documentation. Briefly, in addition to `ORG`s and `GPE`s, there are also `LOC`ations, `PERSON`s, `DATE`s and `TIME`s, `MONEY`, and a few more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visualize entities in context\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style='ent') # Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noun chunks\n",
    "\n",
    "A noun chunk (also called a \"noun phrase\") is one or more of words in sequence that collectively behave like a noun. Typically, they contain a noun plus one or more adjectives and determiners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Noun chunks\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a feature matrix\n",
    "\n",
    "Let's use these new features as, well, *features* like the token counts produced by `CountVectorizer`.\n",
    "\n",
    "From just three (very) short stories, we're going to collect counts of all the distinct named entities and parts of speech, as well as the total length of each document. Then, we'll turn that data into a document-feature matrix.\n",
    "\n",
    "Here's what one of the stories looks like:\n",
    "\n",
    "> From Austin, we headed west. For two weeks, my best friend and I were on a journey of self-exploration, an adventure through the vast American landscape that would find us in a multicolored haze, and an event that would bond us for life. All of this came about because of one thing: a burrito so good we had to drive to California to get it. If it weren’t for that burrito we wouldn’t have found ourselves in a purple sunset, sitting on the edge of the Grand Canyon with a question that would change our lives forever. She said yes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Three tiny love stories from the New York Times\n",
    "# See https://www.nytimes.com/column/modern-love\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "stories = {\n",
    "    'burrito':\"\"\"From Austin, we headed west. For two weeks, my best friend and I were on a journey of self-exploration, an adventure through the vast American landscape that would find us in a multicolored haze, and an event that would bond us for life. All of this came about because of one thing: a burrito so good we had to drive to California to get it. If it weren’t for that burrito we wouldn’t have found ourselves in a purple sunset, sitting on the edge of the Grand Canyon with a question that would change our lives forever. She said yes.\"\"\",\n",
    "    'tripod':\"\"\"On the eve of the new millennium, I fell in love with Andrew, a dashing English ad executive. Inconveniently, I didn’t fall out of love with Scott, an American architectural photographer and my longtime partner. Our dilemma resulted in an unexpected and enduring romance: a V-shaped love triangle sans vows and offspring. Born English, now a naturalized American, I am the hinge in our harmonious household of three: I sleep with both men, they each sleep only with me. We share everything else: home, finances, friends, vacations, life-threatening calamities. As Scott says, our tripod is more stable than a bipod.\"\"\",\n",
    "    'skating':\"\"\"I flew to Idaho over winter break to see Sumner’s hometown. Our first night, we went skating on a frozen pond, surrounded by snow. I was nervous. I didn’t play sports growing up, and I hadn’t ice skated since I was a child. He circled the pond, not showing off, simply enjoying the movement. I’ll never forget the stars piercing the darkness and the shadowy outline of the towering mountains. Fifteen minutes later, I realized I had forgotten that I was supposed to learn how to skate; I had just been watching him the whole time.\"\"\"\n",
    "}\n",
    "\n",
    "story_data = []\n",
    "\n",
    "for story in stories:\n",
    "    counts = defaultdict(int)\n",
    "    doc = nlp(stories[story])\n",
    "    counts['wordcount'] = len(doc) #\n",
    "    for entity in doc.ents:\n",
    "        counts[entity.text+'__'+entity.label_] += 1\n",
    "    for token in doc:\n",
    "        counts[token.pos_] += 1\n",
    "    story_data.append(counts)\n",
    "\n",
    "display(story_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "X = vectorizer.fit_transform(story_data)\n",
    "display(X)\n",
    "display(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select just the GPEs and LOCs, as a sub-example\n",
    "vocab = vectorizer.vocabulary_ # mapping features to index positions\n",
    "\n",
    "# Find indices of places in feature matrix\n",
    "# Iterate over feature names, saving index positions for features that have GPE or LOC in their name\n",
    "idx = [vocab[feature] for feature in vectorizer.feature_names_ if '__GPE' in feature or '__LOC' in feature]\n",
    "# Get feature names in the desired index positions\n",
    "place_names = [vectorizer.get_feature_names_out()[i] for i in idx]\n",
    "# Restrict feature matrix to desired columns\n",
    "X_place = X[:,idx] # <- all rows, columns from list\n",
    "display(X_place)\n",
    "display(place_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed features\n",
    "\n",
    "* If we care about these individual features, we can proceed to analyze them (see Evans and Wilkens, for example)\n",
    "* If we want to use any of these features (*n*-grams, POS counts, entities, whatever) *instead* of token unigrams as part of another workflow (classification, say), we can do that.\n",
    "    * And we can compare the resulting accuracy to that achieved with unigram counts\n",
    "    * Note that we'll still want to scale features, reduce dimensions, select most-informative features, examine feature importances, and so on\n",
    "    * The point is that these *are features* just like unigram counts\n",
    "        * They may (or may not) behave differently in practice (there are relatively few POS types, for instance, so their counts are often higher than wordcounts), but *as features*, we compute with them in just the same way\n",
    "* Maybe best of all, we can use any of these features, or even non-textual features, *alongside* unigram counts\n",
    "    * We can do this as part of feature engineering for classification\n",
    "        * Task is to find the best mix of features for our classification problem\n",
    "    * Or we can specify the feature mix in advance for unsupervised problems\n",
    "        * As always, for unsupervised tasks, we have to specify in advance the set of maximally relevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note: stacking arrays\n",
    "\n",
    "To join features from different matrices, you can use `numpy`'s `hstack` method, like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Synthetic data\n",
    "a = np.array(np.zeros(6)).reshape(3,2)\n",
    "print('a\\n',a)\n",
    "\n",
    "b = np.array(np.ones(6)).reshape(3,2)\n",
    "print('\\nb\\n', b)\n",
    "\n",
    "c = np.hstack([a,b])\n",
    "print('\\nc (stacked)\\n', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add synthetic data columns to right side of feature array\n",
    "X_stacked = np.hstack([X,a])\n",
    "print(X_stacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you need to keep track of your feature names when you do this. The vectorizer object will still give you the names of the columns that it produced, but you might now have multiple vectorizers, each of which is responsible for part of your stacked feature matrix. This isn't a problem, but be aware that you'll need to deal with it. \n",
    "\n",
    "Of course, you need to have ordered your documents in the same way through each of the different vectorization steps if you're going to stack them as we've done here (so that you aren't mixing features from different documents within each row of the feature matrix). If you're using Pandas (rather than NumPy) to store your features, you could use `join` or `merge` to make sure that your feature data is proerly aligned by object identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
